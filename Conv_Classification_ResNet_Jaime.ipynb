{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv Classification ResNet - Jaime.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_pycsS-ECiS_FoUrEBFp_h4chR695McH",
      "authorship_tag": "ABX9TyO4+tvykEKc0v2jNVSxas9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldofranco91/Trading/blob/main/Conv_Classification_ResNet_Jaime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yjg4WnSaOdW",
        "outputId": "5ba08040-ff92-4b6f-a831-3fd69ba996d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb  9 20:02:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAYfXm-rbF6R",
        "outputId": "5f833bae-d96c-45b2-8c27-e181afcfa03c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.7.1)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "# importing the requests library \n",
        "import requests \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "# Viz\n",
        "import matplotlib.pyplot as plt # basic plotting\n",
        "import seaborn as sns # for prettier plots\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from pickle import dump, load\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pNF6gGgrbNmM",
        "outputId": "c03b3af3-3688-4927-e362-9becfc1641a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "8l67Hhl1bQB-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data"
      ],
      "metadata": {
        "id": "EUELOSlCbWJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder = '/content/drive/MyDrive/Trading/7_days_class/'\n",
        "# ajustar valores\n",
        "past_t = 7\n",
        "forecast_t = 1"
      ],
      "metadata": {
        "id": "ZdSbWekNqk3F"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "start = dt.datetime(2018,2,1)\n",
        "end = dt.datetime.now() - dt.timedelta(days=1)\n",
        "\n",
        "df = yf.download('BTC-USD', start, end).drop('Adj Close', axis=1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "m8KEST-6bT_z",
        "outputId": "4af9d1af-2630-48b0-ff61-1725a0627580"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5f687e62-45af-4850-acdd-79a7a30d7f15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-02-01</th>\n",
              "      <td>10237.299805</td>\n",
              "      <td>10288.799805</td>\n",
              "      <td>8812.280273</td>\n",
              "      <td>9170.540039</td>\n",
              "      <td>9959400448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-02</th>\n",
              "      <td>9142.280273</td>\n",
              "      <td>9142.280273</td>\n",
              "      <td>7796.490234</td>\n",
              "      <td>8830.750000</td>\n",
              "      <td>12726899712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-03</th>\n",
              "      <td>8852.120117</td>\n",
              "      <td>9430.750000</td>\n",
              "      <td>8251.629883</td>\n",
              "      <td>9174.910156</td>\n",
              "      <td>7263790080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-04</th>\n",
              "      <td>9175.700195</td>\n",
              "      <td>9334.870117</td>\n",
              "      <td>8031.220215</td>\n",
              "      <td>8277.009766</td>\n",
              "      <td>7073549824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-05</th>\n",
              "      <td>8270.540039</td>\n",
              "      <td>8364.839844</td>\n",
              "      <td>6756.680176</td>\n",
              "      <td>6955.270020</td>\n",
              "      <td>9285289984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-04</th>\n",
              "      <td>37149.265625</td>\n",
              "      <td>41527.785156</td>\n",
              "      <td>37093.628906</td>\n",
              "      <td>41500.875000</td>\n",
              "      <td>29412210792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-05</th>\n",
              "      <td>41501.480469</td>\n",
              "      <td>41847.164062</td>\n",
              "      <td>41038.097656</td>\n",
              "      <td>41441.164062</td>\n",
              "      <td>19652846215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-06</th>\n",
              "      <td>41441.121094</td>\n",
              "      <td>42500.785156</td>\n",
              "      <td>41244.906250</td>\n",
              "      <td>42412.433594</td>\n",
              "      <td>16142097334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-07</th>\n",
              "      <td>42406.781250</td>\n",
              "      <td>44401.863281</td>\n",
              "      <td>41748.156250</td>\n",
              "      <td>43840.285156</td>\n",
              "      <td>28641855926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-08</th>\n",
              "      <td>43854.652344</td>\n",
              "      <td>45293.867188</td>\n",
              "      <td>42807.835938</td>\n",
              "      <td>44118.445312</td>\n",
              "      <td>33079398868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1469 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f687e62-45af-4850-acdd-79a7a30d7f15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f687e62-45af-4850-acdd-79a7a30d7f15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f687e62-45af-4850-acdd-79a7a30d7f15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    Open          High           Low         Close  \\\n",
              "Date                                                                 \n",
              "2018-02-01  10237.299805  10288.799805   8812.280273   9170.540039   \n",
              "2018-02-02   9142.280273   9142.280273   7796.490234   8830.750000   \n",
              "2018-02-03   8852.120117   9430.750000   8251.629883   9174.910156   \n",
              "2018-02-04   9175.700195   9334.870117   8031.220215   8277.009766   \n",
              "2018-02-05   8270.540039   8364.839844   6756.680176   6955.270020   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-04  37149.265625  41527.785156  37093.628906  41500.875000   \n",
              "2022-02-05  41501.480469  41847.164062  41038.097656  41441.164062   \n",
              "2022-02-06  41441.121094  42500.785156  41244.906250  42412.433594   \n",
              "2022-02-07  42406.781250  44401.863281  41748.156250  43840.285156   \n",
              "2022-02-08  43854.652344  45293.867188  42807.835938  44118.445312   \n",
              "\n",
              "                 Volume  \n",
              "Date                     \n",
              "2018-02-01   9959400448  \n",
              "2018-02-02  12726899712  \n",
              "2018-02-03   7263790080  \n",
              "2018-02-04   7073549824  \n",
              "2018-02-05   9285289984  \n",
              "...                 ...  \n",
              "2022-02-04  29412210792  \n",
              "2022-02-05  19652846215  \n",
              "2022-02-06  16142097334  \n",
              "2022-02-07  28641855926  \n",
              "2022-02-08  33079398868  \n",
              "\n",
              "[1469 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indicators"
      ],
      "metadata": {
        "id": "u5nJHwJmbcJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fear index\n",
        "df_fear = pd.read_csv('/content/drive/MyDrive/Trading/fear_index.csv')\n",
        "df_fear = df_fear[df_fear.date.between('2018-02-01', '2022-01-31')]\n",
        "df_fear['date'] = pd.to_datetime(df_fear['date'])\n",
        "df['date'] = pd.to_datetime(df.index)\n",
        "df = df.join(df_fear.set_index('date'), on='date')\n",
        "df = df.drop(['date'], axis=1)\n",
        "df['value'] = df['value'].fillna(25) # Fill 3 consequtive days taking sorrounding values (23-26)"
      ],
      "metadata": {
        "id": "zNGXCRkcbdRN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price = 'Close'\n",
        "\n",
        "# MovingAverage\n",
        "#https://towardsdatascience.com/building-a-comprehensive-set-of-technical-indicators-in-python-for-quantitative-trading-8d98751b5fb\n",
        "values_ma = range(2,30,2)\n",
        "for i in values_ma:\n",
        "    df[\"maclose\" + str(i)] = df[price].rolling(window=i).mean()\n",
        "\n",
        "# ExponentialMovingWindow\n",
        "values_ema = range(2,30,2)\n",
        "for i in values_ema:\n",
        "    df[\"ema\" + str(i)] = df[price].ewm(span = i, adjust = False).mean()\n",
        "\n",
        "# Wight Moving Average\n",
        "# https://predictivehacks.com/?all-tips=how-to-apply-a-rolling-weighted-moving-average-in-pandas\n",
        "weights = np.array([0.1, 0.2, 0.3, 0.4]) #Tienen que sumar 1 🙂\n",
        "df['wma'] = df[price].rolling(4).apply(lambda x: np.sum(weights*x))\n",
        "\n",
        "# RSI\n",
        "# https://stackoverflow.com/questions/20526414/relative-strength-index-in-python-pandas\n",
        "values_rsi = range(2,30,2)\n",
        "close = df[price]\n",
        "\n",
        "# Get the difference in price from previous step\n",
        "delta = close.diff()\n",
        "\n",
        "# Make the positive gains (up) and negative gains (down) Series\n",
        "up, down = delta.copy(), delta.copy()\n",
        "up[up < 0] = 0\n",
        "down[down > 0] = 0\n",
        "\n",
        "for i in values_rsi:\n",
        "    # Calculate the EWMA\n",
        "    roll_up1 = up.ewm(span=i).mean()\n",
        "    roll_down1 = down.abs().ewm(span=i).mean()\n",
        "\n",
        "    ## Calculate the RSI based on EWMA\n",
        "    RS1 = roll_up1 / roll_down1\n",
        "    RSI1 = 100.0 - (100.0 / (1.0 + RS1))\n",
        "\n",
        "    # Calculate the SMA\n",
        "    roll_up2 = up.rolling(i).mean()\n",
        "    roll_down2 = down.abs().rolling(i).mean()\n",
        "\n",
        "    # Calculate the RSI based on SMA\n",
        "    RS2 = roll_up2 / roll_down2\n",
        "    RSI2 = 100.0 - (100.0 / (1.0 + RS2))\n",
        "\n",
        "    df[\"rsiewma\" + str(i)] = RSI1.values\n",
        "    df[\"rsisma\" + str(i)]  = RSI2.values\n",
        "\n",
        "# BollingerBand\n",
        "values_bollinger = range(2,30,2)\n",
        "for i in values_bollinger:\n",
        "    df['bollingerupper' + str(i)] = df[price].rolling(window=i).mean() + (df[price].rolling(window=i).std() * 2)\n",
        "    df['bollingerlower' + str(i)] = df[price].rolling(window=i).mean() - (df[price].rolling(window=i).std() * 2)"
      ],
      "metadata": {
        "id": "Juvqu97Obetx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diff'] = df['Close'].shift(-forecast_t) - df['Close'] \n",
        "df['percen_diff'] = (df['diff']/df['Close'])*100 "
      ],
      "metadata": {
        "id": "mh1Y4kYADayI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.percen_diff.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqoNArEvDfMF",
        "outputId": "a1082139-64bb-4bff-eb6a-1407cfb1b863"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1468.000000\n",
              "mean        0.183195\n",
              "std         3.869176\n",
              "min       -37.169539\n",
              "25%        -1.529662\n",
              "50%         0.143351\n",
              "75%         1.796651\n",
              "max        18.746474\n",
              "Name: percen_diff, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target(x):\n",
        "  # reducir margenes\n",
        "  if x<-1:\n",
        "    return \"sell\"\n",
        "  elif x>1:\n",
        "    return \"buy\"\n",
        "  else:\n",
        "    return \"hold\"\n",
        "\n",
        "df['target'] = df['percen_diff'].apply(lambda x: get_target(x))\n",
        "df = df.join(pd.get_dummies(df.target))"
      ],
      "metadata": {
        "id": "ty6VmvJcgFal"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df.tail(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "U49fzTcXb5KA",
        "outputId": "7edf64ed-56e2-4310-e9e5-c77e1dbe136f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4566aa7b-97df-4778-a033-7e3886195b93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>value</th>\n",
              "      <th>maclose2</th>\n",
              "      <th>maclose4</th>\n",
              "      <th>maclose6</th>\n",
              "      <th>maclose8</th>\n",
              "      <th>maclose10</th>\n",
              "      <th>maclose12</th>\n",
              "      <th>maclose14</th>\n",
              "      <th>maclose16</th>\n",
              "      <th>maclose18</th>\n",
              "      <th>maclose20</th>\n",
              "      <th>maclose22</th>\n",
              "      <th>maclose24</th>\n",
              "      <th>maclose26</th>\n",
              "      <th>maclose28</th>\n",
              "      <th>ema2</th>\n",
              "      <th>ema4</th>\n",
              "      <th>ema6</th>\n",
              "      <th>ema8</th>\n",
              "      <th>ema10</th>\n",
              "      <th>ema12</th>\n",
              "      <th>ema14</th>\n",
              "      <th>ema16</th>\n",
              "      <th>ema18</th>\n",
              "      <th>ema20</th>\n",
              "      <th>ema22</th>\n",
              "      <th>ema24</th>\n",
              "      <th>ema26</th>\n",
              "      <th>ema28</th>\n",
              "      <th>wma</th>\n",
              "      <th>rsiewma2</th>\n",
              "      <th>rsisma2</th>\n",
              "      <th>rsiewma4</th>\n",
              "      <th>rsisma4</th>\n",
              "      <th>rsiewma6</th>\n",
              "      <th>rsisma6</th>\n",
              "      <th>rsiewma8</th>\n",
              "      <th>rsisma8</th>\n",
              "      <th>rsiewma10</th>\n",
              "      <th>rsisma10</th>\n",
              "      <th>rsiewma12</th>\n",
              "      <th>rsisma12</th>\n",
              "      <th>rsiewma14</th>\n",
              "      <th>rsisma14</th>\n",
              "      <th>rsiewma16</th>\n",
              "      <th>rsisma16</th>\n",
              "      <th>rsiewma18</th>\n",
              "      <th>rsisma18</th>\n",
              "      <th>rsiewma20</th>\n",
              "      <th>rsisma20</th>\n",
              "      <th>rsiewma22</th>\n",
              "      <th>rsisma22</th>\n",
              "      <th>rsiewma24</th>\n",
              "      <th>rsisma24</th>\n",
              "      <th>rsiewma26</th>\n",
              "      <th>rsisma26</th>\n",
              "      <th>rsiewma28</th>\n",
              "      <th>rsisma28</th>\n",
              "      <th>bollingerupper2</th>\n",
              "      <th>bollingerlower2</th>\n",
              "      <th>bollingerupper4</th>\n",
              "      <th>bollingerlower4</th>\n",
              "      <th>bollingerupper6</th>\n",
              "      <th>bollingerlower6</th>\n",
              "      <th>bollingerupper8</th>\n",
              "      <th>bollingerlower8</th>\n",
              "      <th>bollingerupper10</th>\n",
              "      <th>bollingerlower10</th>\n",
              "      <th>bollingerupper12</th>\n",
              "      <th>bollingerlower12</th>\n",
              "      <th>bollingerupper14</th>\n",
              "      <th>bollingerlower14</th>\n",
              "      <th>bollingerupper16</th>\n",
              "      <th>bollingerlower16</th>\n",
              "      <th>bollingerupper18</th>\n",
              "      <th>bollingerlower18</th>\n",
              "      <th>bollingerupper20</th>\n",
              "      <th>bollingerlower20</th>\n",
              "      <th>bollingerupper22</th>\n",
              "      <th>bollingerlower22</th>\n",
              "      <th>bollingerupper24</th>\n",
              "      <th>bollingerlower24</th>\n",
              "      <th>bollingerupper26</th>\n",
              "      <th>bollingerlower26</th>\n",
              "      <th>bollingerupper28</th>\n",
              "      <th>bollingerlower28</th>\n",
              "      <th>diff</th>\n",
              "      <th>percen_diff</th>\n",
              "      <th>target</th>\n",
              "      <th>buy</th>\n",
              "      <th>hold</th>\n",
              "      <th>sell</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-10-31</th>\n",
              "      <td>61850.488281</td>\n",
              "      <td>62406.171875</td>\n",
              "      <td>60074.328125</td>\n",
              "      <td>61318.957031</td>\n",
              "      <td>32241199927</td>\n",
              "      <td>74.0</td>\n",
              "      <td>61603.894531</td>\n",
              "      <td>61514.472656</td>\n",
              "      <td>60817.345052</td>\n",
              "      <td>61109.341309</td>\n",
              "      <td>61096.061328</td>\n",
              "      <td>61596.968424</td>\n",
              "      <td>61817.977958</td>\n",
              "      <td>61743.593018</td>\n",
              "      <td>61489.608941</td>\n",
              "      <td>61012.755859</td>\n",
              "      <td>60568.703835</td>\n",
              "      <td>60060.314779</td>\n",
              "      <td>59639.038011</td>\n",
              "      <td>58972.953683</td>\n",
              "      <td>61471.231054</td>\n",
              "      <td>61386.530841</td>\n",
              "      <td>61323.459532</td>\n",
              "      <td>61271.284927</td>\n",
              "      <td>61166.798553</td>\n",
              "      <td>60990.749539</td>\n",
              "      <td>60749.164121</td>\n",
              "      <td>60457.043581</td>\n",
              "      <td>60130.242404</td>\n",
              "      <td>59782.278861</td>\n",
              "      <td>59423.547610</td>\n",
              "      <td>59061.540016</td>\n",
              "      <td>58701.393936</td>\n",
              "      <td>58346.475249</td>\n",
              "      <td>61602.039062</td>\n",
              "      <td>26.649901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.693152</td>\n",
              "      <td>80.470705</td>\n",
              "      <td>49.923215</td>\n",
              "      <td>40.659668</td>\n",
              "      <td>50.529448</td>\n",
              "      <td>49.683208</td>\n",
              "      <td>51.247001</td>\n",
              "      <td>46.817783</td>\n",
              "      <td>52.130428</td>\n",
              "      <td>42.460159</td>\n",
              "      <td>53.063933</td>\n",
              "      <td>49.472079</td>\n",
              "      <td>53.959510</td>\n",
              "      <td>49.417097</td>\n",
              "      <td>54.770702</td>\n",
              "      <td>57.011166</td>\n",
              "      <td>55.479988</td>\n",
              "      <td>56.235654</td>\n",
              "      <td>56.086492</td>\n",
              "      <td>59.435399</td>\n",
              "      <td>56.597705</td>\n",
              "      <td>60.789539</td>\n",
              "      <td>57.024544</td>\n",
              "      <td>62.188720</td>\n",
              "      <td>57.378653</td>\n",
              "      <td>65.067896</td>\n",
              "      <td>62409.819485</td>\n",
              "      <td>60797.969577</td>\n",
              "      <td>62920.984099</td>\n",
              "      <td>60107.961214</td>\n",
              "      <td>63513.326257</td>\n",
              "      <td>58121.363847</td>\n",
              "      <td>63871.906642</td>\n",
              "      <td>58346.775975</td>\n",
              "      <td>63555.383183</td>\n",
              "      <td>58636.739474</td>\n",
              "      <td>65205.914018</td>\n",
              "      <td>57988.022831</td>\n",
              "      <td>65430.775579</td>\n",
              "      <td>58205.180336</td>\n",
              "      <td>65140.001150</td>\n",
              "      <td>58347.184885</td>\n",
              "      <td>65299.021618</td>\n",
              "      <td>57680.196264</td>\n",
              "      <td>65681.335136</td>\n",
              "      <td>56344.176583</td>\n",
              "      <td>65924.432322</td>\n",
              "      <td>55212.975349</td>\n",
              "      <td>66236.337987</td>\n",
              "      <td>53884.291570</td>\n",
              "      <td>66283.198391</td>\n",
              "      <td>52994.877631</td>\n",
              "      <td>67049.304882</td>\n",
              "      <td>50896.602484</td>\n",
              "      <td>-314.550781</td>\n",
              "      <td>-0.512975</td>\n",
              "      <td>hold</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-01</th>\n",
              "      <td>61320.449219</td>\n",
              "      <td>62419.003906</td>\n",
              "      <td>59695.183594</td>\n",
              "      <td>61004.406250</td>\n",
              "      <td>36150572843</td>\n",
              "      <td>74.0</td>\n",
              "      <td>61161.681641</td>\n",
              "      <td>61610.040039</td>\n",
              "      <td>60924.113932</td>\n",
              "      <td>61118.537598</td>\n",
              "      <td>61127.275391</td>\n",
              "      <td>61181.265951</td>\n",
              "      <td>61745.001395</td>\n",
              "      <td>61750.607178</td>\n",
              "      <td>61694.213542</td>\n",
              "      <td>61260.923242</td>\n",
              "      <td>60852.014205</td>\n",
              "      <td>60353.504720</td>\n",
              "      <td>59856.074820</td>\n",
              "      <td>59397.650251</td>\n",
              "      <td>61160.014518</td>\n",
              "      <td>61233.681005</td>\n",
              "      <td>61232.301451</td>\n",
              "      <td>61211.978554</td>\n",
              "      <td>61137.272680</td>\n",
              "      <td>60992.850571</td>\n",
              "      <td>60783.196405</td>\n",
              "      <td>60521.439189</td>\n",
              "      <td>60222.259651</td>\n",
              "      <td>59898.671945</td>\n",
              "      <td>59561.013579</td>\n",
              "      <td>59216.969315</td>\n",
              "      <td>58871.987441</td>\n",
              "      <td>58529.780836</td>\n",
              "      <td>61398.012500</td>\n",
              "      <td>13.564493</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39.261251</td>\n",
              "      <td>56.755343</td>\n",
              "      <td>45.086107</td>\n",
              "      <td>54.675639</td>\n",
              "      <td>47.229602</td>\n",
              "      <td>50.316144</td>\n",
              "      <td>48.734381</td>\n",
              "      <td>51.219328</td>\n",
              "      <td>50.085169</td>\n",
              "      <td>36.220010</td>\n",
              "      <td>51.327596</td>\n",
              "      <td>47.685071</td>\n",
              "      <td>52.443879</td>\n",
              "      <td>50.241858</td>\n",
              "      <td>53.421911</td>\n",
              "      <td>56.535701</td>\n",
              "      <td>54.262630</td>\n",
              "      <td>58.379863</td>\n",
              "      <td>54.975894</td>\n",
              "      <td>59.227894</td>\n",
              "      <td>55.575853</td>\n",
              "      <td>60.061227</td>\n",
              "      <td>56.077800</td>\n",
              "      <td>57.690882</td>\n",
              "      <td>56.496387</td>\n",
              "      <td>63.848403</td>\n",
              "      <td>61606.523622</td>\n",
              "      <td>60716.839660</td>\n",
              "      <td>62712.206175</td>\n",
              "      <td>60507.873903</td>\n",
              "      <td>63584.381312</td>\n",
              "      <td>58263.846553</td>\n",
              "      <td>63878.875389</td>\n",
              "      <td>58358.199806</td>\n",
              "      <td>63571.697656</td>\n",
              "      <td>58682.853126</td>\n",
              "      <td>63498.875926</td>\n",
              "      <td>58863.655975</td>\n",
              "      <td>65380.892219</td>\n",
              "      <td>58109.110572</td>\n",
              "      <td>65139.969399</td>\n",
              "      <td>58361.244956</td>\n",
              "      <td>64903.881148</td>\n",
              "      <td>58484.545935</td>\n",
              "      <td>65302.283336</td>\n",
              "      <td>57219.563148</td>\n",
              "      <td>65540.550178</td>\n",
              "      <td>56163.478231</td>\n",
              "      <td>65964.578635</td>\n",
              "      <td>54742.430806</td>\n",
              "      <td>66284.102883</td>\n",
              "      <td>53428.046757</td>\n",
              "      <td>66517.171509</td>\n",
              "      <td>52278.128993</td>\n",
              "      <td>2221.996094</td>\n",
              "      <td>3.642353</td>\n",
              "      <td>buy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-02</th>\n",
              "      <td>60963.253906</td>\n",
              "      <td>64242.792969</td>\n",
              "      <td>60673.054688</td>\n",
              "      <td>63226.402344</td>\n",
              "      <td>37746665647</td>\n",
              "      <td>73.0</td>\n",
              "      <td>62115.404297</td>\n",
              "      <td>61859.649414</td>\n",
              "      <td>61714.783203</td>\n",
              "      <td>61141.859863</td>\n",
              "      <td>61310.553906</td>\n",
              "      <td>61265.951823</td>\n",
              "      <td>61671.030692</td>\n",
              "      <td>61855.156250</td>\n",
              "      <td>61784.905382</td>\n",
              "      <td>61552.188477</td>\n",
              "      <td>61112.996626</td>\n",
              "      <td>60697.595540</td>\n",
              "      <td>60218.398588</td>\n",
              "      <td>59815.921317</td>\n",
              "      <td>62537.606402</td>\n",
              "      <td>62030.769540</td>\n",
              "      <td>61802.044563</td>\n",
              "      <td>61659.628285</td>\n",
              "      <td>61517.114437</td>\n",
              "      <td>61336.473921</td>\n",
              "      <td>61108.957197</td>\n",
              "      <td>60839.670148</td>\n",
              "      <td>60538.485197</td>\n",
              "      <td>60215.598650</td>\n",
              "      <td>59879.743037</td>\n",
              "      <td>59537.723957</td>\n",
              "      <td>59194.536693</td>\n",
              "      <td>58853.685767</td>\n",
              "      <td>62044.557422</td>\n",
              "      <td>92.421634</td>\n",
              "      <td>87.599252</td>\n",
              "      <td>80.289052</td>\n",
              "      <td>64.488777</td>\n",
              "      <td>71.957194</td>\n",
              "      <td>82.985176</td>\n",
              "      <td>66.876168</td>\n",
              "      <td>50.794046</td>\n",
              "      <td>63.981473</td>\n",
              "      <td>56.399238</td>\n",
              "      <td>62.400419</td>\n",
              "      <td>53.072114</td>\n",
              "      <td>61.575650</td>\n",
              "      <td>47.652054</td>\n",
              "      <td>61.174622</td>\n",
              "      <td>53.377797</td>\n",
              "      <td>61.003780</td>\n",
              "      <td>53.124350</td>\n",
              "      <td>60.952180</td>\n",
              "      <td>59.556985</td>\n",
              "      <td>60.957257</td>\n",
              "      <td>58.626103</td>\n",
              "      <td>60.984526</td>\n",
              "      <td>61.409379</td>\n",
              "      <td>61.015645</td>\n",
              "      <td>62.610140</td>\n",
              "      <td>61.041437</td>\n",
              "      <td>63.696275</td>\n",
              "      <td>65257.781308</td>\n",
              "      <td>58973.027286</td>\n",
              "      <td>63823.541576</td>\n",
              "      <td>59895.757252</td>\n",
              "      <td>63598.148474</td>\n",
              "      <td>59831.417932</td>\n",
              "      <td>63978.504093</td>\n",
              "      <td>58305.215634</td>\n",
              "      <td>64094.929866</td>\n",
              "      <td>58526.177947</td>\n",
              "      <td>63810.750018</td>\n",
              "      <td>58721.153628</td>\n",
              "      <td>65123.866074</td>\n",
              "      <td>58218.195310</td>\n",
              "      <td>65320.929437</td>\n",
              "      <td>58389.383063</td>\n",
              "      <td>65073.847838</td>\n",
              "      <td>58495.962926</td>\n",
              "      <td>65247.076980</td>\n",
              "      <td>57857.299973</td>\n",
              "      <td>65652.946154</td>\n",
              "      <td>56573.047099</td>\n",
              "      <td>65930.344909</td>\n",
              "      <td>55464.846172</td>\n",
              "      <td>66279.282045</td>\n",
              "      <td>54157.515130</td>\n",
              "      <td>66367.853769</td>\n",
              "      <td>53263.988865</td>\n",
              "      <td>-256.355469</td>\n",
              "      <td>-0.405456</td>\n",
              "      <td>hold</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-03</th>\n",
              "      <td>63254.335938</td>\n",
              "      <td>63516.937500</td>\n",
              "      <td>61184.238281</td>\n",
              "      <td>62970.046875</td>\n",
              "      <td>36124731509</td>\n",
              "      <td>76.0</td>\n",
              "      <td>63098.224609</td>\n",
              "      <td>62129.953125</td>\n",
              "      <td>62106.101562</td>\n",
              "      <td>61467.641602</td>\n",
              "      <td>61514.475000</td>\n",
              "      <td>61455.766927</td>\n",
              "      <td>61455.117188</td>\n",
              "      <td>61914.154297</td>\n",
              "      <td>61900.342448</td>\n",
              "      <td>61834.614648</td>\n",
              "      <td>61427.950639</td>\n",
              "      <td>61039.198405</td>\n",
              "      <td>60564.637019</td>\n",
              "      <td>60087.656948</td>\n",
              "      <td>62825.900051</td>\n",
              "      <td>62406.480474</td>\n",
              "      <td>62135.759510</td>\n",
              "      <td>61950.832416</td>\n",
              "      <td>61781.283971</td>\n",
              "      <td>61587.792837</td>\n",
              "      <td>61357.102487</td>\n",
              "      <td>61090.302704</td>\n",
              "      <td>60794.439058</td>\n",
              "      <td>60477.927052</td>\n",
              "      <td>60148.465110</td>\n",
              "      <td>59812.309791</td>\n",
              "      <td>59474.204114</td>\n",
              "      <td>59137.572740</td>\n",
              "      <td>62488.716406</td>\n",
              "      <td>70.241538</td>\n",
              "      <td>89.656210</td>\n",
              "      <td>71.059463</td>\n",
              "      <td>66.076218</td>\n",
              "      <td>66.686495</td>\n",
              "      <td>72.117803</td>\n",
              "      <td>63.376183</td>\n",
              "      <td>63.968718</td>\n",
              "      <td>61.406204</td>\n",
              "      <td>57.224116</td>\n",
              "      <td>60.369551</td>\n",
              "      <td>57.454426</td>\n",
              "      <td>59.896809</td>\n",
              "      <td>42.655510</td>\n",
              "      <td>59.740550</td>\n",
              "      <td>51.922906</td>\n",
              "      <td>59.749675</td>\n",
              "      <td>54.045806</td>\n",
              "      <td>59.836204</td>\n",
              "      <td>59.213514</td>\n",
              "      <td>59.950885</td>\n",
              "      <td>60.795143</td>\n",
              "      <td>60.067415</td>\n",
              "      <td>61.308225</td>\n",
              "      <td>60.172745</td>\n",
              "      <td>62.019907</td>\n",
              "      <td>60.261274</td>\n",
              "      <td>59.713673</td>\n",
              "      <td>63460.765990</td>\n",
              "      <td>62735.683229</td>\n",
              "      <td>64390.492538</td>\n",
              "      <td>59869.413712</td>\n",
              "      <td>63871.742778</td>\n",
              "      <td>60340.460347</td>\n",
              "      <td>64488.452569</td>\n",
              "      <td>58446.830634</td>\n",
              "      <td>64468.762234</td>\n",
              "      <td>58560.187766</td>\n",
              "      <td>64149.291884</td>\n",
              "      <td>58762.241970</td>\n",
              "      <td>64003.330200</td>\n",
              "      <td>58906.904175</td>\n",
              "      <td>65424.197552</td>\n",
              "      <td>58404.111042</td>\n",
              "      <td>65202.412880</td>\n",
              "      <td>58598.272016</td>\n",
              "      <td>64992.377345</td>\n",
              "      <td>58676.851952</td>\n",
              "      <td>65422.000060</td>\n",
              "      <td>57433.901219</td>\n",
              "      <td>65695.943144</td>\n",
              "      <td>56382.453666</td>\n",
              "      <td>66149.970682</td>\n",
              "      <td>54979.303356</td>\n",
              "      <td>66502.933121</td>\n",
              "      <td>53672.380774</td>\n",
              "      <td>-1517.816406</td>\n",
              "      <td>-2.410378</td>\n",
              "      <td>sell</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-04</th>\n",
              "      <td>62941.804688</td>\n",
              "      <td>63123.289062</td>\n",
              "      <td>60799.664062</td>\n",
              "      <td>61452.230469</td>\n",
              "      <td>32615846901</td>\n",
              "      <td>73.0</td>\n",
              "      <td>62211.138672</td>\n",
              "      <td>62163.271484</td>\n",
              "      <td>61976.812500</td>\n",
              "      <td>61838.872070</td>\n",
              "      <td>61355.715625</td>\n",
              "      <td>61460.651367</td>\n",
              "      <td>61400.978516</td>\n",
              "      <td>61738.544189</td>\n",
              "      <td>61894.709852</td>\n",
              "      <td>61827.528711</td>\n",
              "      <td>61612.093040</td>\n",
              "      <td>61204.508464</td>\n",
              "      <td>60814.021935</td>\n",
              "      <td>60360.737165</td>\n",
              "      <td>61910.120329</td>\n",
              "      <td>62024.780472</td>\n",
              "      <td>61940.465498</td>\n",
              "      <td>61840.031983</td>\n",
              "      <td>61721.456062</td>\n",
              "      <td>61566.937088</td>\n",
              "      <td>61369.786218</td>\n",
              "      <td>61132.882441</td>\n",
              "      <td>60863.680259</td>\n",
              "      <td>60570.717854</td>\n",
              "      <td>60261.836011</td>\n",
              "      <td>59943.503445</td>\n",
              "      <td>59620.724585</td>\n",
              "      <td>59297.204308</td>\n",
              "      <td>62217.627344</td>\n",
              "      <td>13.346981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.293079</td>\n",
              "      <td>51.545838</td>\n",
              "      <td>41.493537</td>\n",
              "      <td>42.569205</td>\n",
              "      <td>45.320600</td>\n",
              "      <td>66.562983</td>\n",
              "      <td>47.554906</td>\n",
              "      <td>44.129912</td>\n",
              "      <td>49.171661</td>\n",
              "      <td>50.182091</td>\n",
              "      <td>50.492043</td>\n",
              "      <td>47.930679</td>\n",
              "      <td>51.620541</td>\n",
              "      <td>44.103897</td>\n",
              "      <td>52.594807</td>\n",
              "      <td>49.808961</td>\n",
              "      <td>53.434052</td>\n",
              "      <td>49.746013</td>\n",
              "      <td>54.153347</td>\n",
              "      <td>56.280659</td>\n",
              "      <td>54.766996</td>\n",
              "      <td>55.658941</td>\n",
              "      <td>55.288780</td>\n",
              "      <td>58.539570</td>\n",
              "      <td>55.731544</td>\n",
              "      <td>59.771130</td>\n",
              "      <td>64357.655219</td>\n",
              "      <td>60064.622125</td>\n",
              "      <td>64363.175031</td>\n",
              "      <td>59963.367937</td>\n",
              "      <td>63811.863214</td>\n",
              "      <td>60141.761786</td>\n",
              "      <td>63683.596963</td>\n",
              "      <td>59994.147177</td>\n",
              "      <td>64109.518502</td>\n",
              "      <td>58601.912748</td>\n",
              "      <td>64153.897096</td>\n",
              "      <td>58767.405638</td>\n",
              "      <td>63912.023831</td>\n",
              "      <td>58889.933200</td>\n",
              "      <td>65021.190358</td>\n",
              "      <td>58455.898021</td>\n",
              "      <td>65199.629846</td>\n",
              "      <td>58589.789859</td>\n",
              "      <td>64988.199976</td>\n",
              "      <td>58666.857446</td>\n",
              "      <td>65178.852829</td>\n",
              "      <td>58045.333251</td>\n",
              "      <td>65609.470552</td>\n",
              "      <td>56799.546375</td>\n",
              "      <td>65918.148195</td>\n",
              "      <td>55709.895676</td>\n",
              "      <td>66300.135421</td>\n",
              "      <td>54421.338909</td>\n",
              "      <td>-326.554688</td>\n",
              "      <td>-0.531396</td>\n",
              "      <td>hold</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-03</th>\n",
              "      <td>36944.804688</td>\n",
              "      <td>37154.601562</td>\n",
              "      <td>36375.539062</td>\n",
              "      <td>37154.601562</td>\n",
              "      <td>18591534769</td>\n",
              "      <td>25.0</td>\n",
              "      <td>37053.792969</td>\n",
              "      <td>37833.496094</td>\n",
              "      <td>37898.294271</td>\n",
              "      <td>37789.041504</td>\n",
              "      <td>37611.845703</td>\n",
              "      <td>37420.799154</td>\n",
              "      <td>37181.225446</td>\n",
              "      <td>37685.118896</td>\n",
              "      <td>38199.338108</td>\n",
              "      <td>38693.968164</td>\n",
              "      <td>39071.392401</td>\n",
              "      <td>39427.316243</td>\n",
              "      <td>39614.940505</td>\n",
              "      <td>39760.010603</td>\n",
              "      <td>37269.780741</td>\n",
              "      <td>37527.447337</td>\n",
              "      <td>37625.775879</td>\n",
              "      <td>37711.027442</td>\n",
              "      <td>37834.693349</td>\n",
              "      <td>38000.255509</td>\n",
              "      <td>38199.304662</td>\n",
              "      <td>38422.686695</td>\n",
              "      <td>38663.083062</td>\n",
              "      <td>38915.160140</td>\n",
              "      <td>39175.129430</td>\n",
              "      <td>39440.266279</td>\n",
              "      <td>39708.533779</td>\n",
              "      <td>39978.327368</td>\n",
              "      <td>37544.703125</td>\n",
              "      <td>29.757072</td>\n",
              "      <td>10.121821</td>\n",
              "      <td>30.670476</td>\n",
              "      <td>36.460003</td>\n",
              "      <td>35.141693</td>\n",
              "      <td>40.717427</td>\n",
              "      <td>37.503237</td>\n",
              "      <td>53.497519</td>\n",
              "      <td>38.308944</td>\n",
              "      <td>55.293032</td>\n",
              "      <td>38.346414</td>\n",
              "      <td>66.727569</td>\n",
              "      <td>38.069199</td>\n",
              "      <td>35.309122</td>\n",
              "      <td>37.696246</td>\n",
              "      <td>30.938511</td>\n",
              "      <td>37.325422</td>\n",
              "      <td>29.707772</td>\n",
              "      <td>36.996761</td>\n",
              "      <td>29.948896</td>\n",
              "      <td>36.723595</td>\n",
              "      <td>29.645635</td>\n",
              "      <td>36.507402</td>\n",
              "      <td>37.600781</td>\n",
              "      <td>36.344774</td>\n",
              "      <td>38.003634</td>\n",
              "      <td>36.230610</td>\n",
              "      <td>35.606987</td>\n",
              "      <td>37338.922730</td>\n",
              "      <td>36768.663208</td>\n",
              "      <td>39654.086182</td>\n",
              "      <td>36012.906005</td>\n",
              "      <td>39329.552236</td>\n",
              "      <td>36467.036306</td>\n",
              "      <td>39110.472179</td>\n",
              "      <td>36467.610828</td>\n",
              "      <td>38996.995102</td>\n",
              "      <td>36226.696304</td>\n",
              "      <td>38967.423213</td>\n",
              "      <td>35874.175094</td>\n",
              "      <td>39135.913133</td>\n",
              "      <td>35226.537759</td>\n",
              "      <td>41008.624254</td>\n",
              "      <td>34361.613539</td>\n",
              "      <td>42524.526534</td>\n",
              "      <td>33874.149682</td>\n",
              "      <td>43793.946522</td>\n",
              "      <td>33593.989806</td>\n",
              "      <td>44505.225405</td>\n",
              "      <td>33637.559396</td>\n",
              "      <td>45163.461205</td>\n",
              "      <td>33691.171282</td>\n",
              "      <td>45274.363633</td>\n",
              "      <td>33955.517376</td>\n",
              "      <td>45309.220551</td>\n",
              "      <td>34210.800654</td>\n",
              "      <td>4346.273438</td>\n",
              "      <td>11.697807</td>\n",
              "      <td>buy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-04</th>\n",
              "      <td>37149.265625</td>\n",
              "      <td>41527.785156</td>\n",
              "      <td>37093.628906</td>\n",
              "      <td>41500.875000</td>\n",
              "      <td>29412210792</td>\n",
              "      <td>25.0</td>\n",
              "      <td>39327.738281</td>\n",
              "      <td>38587.933594</td>\n",
              "      <td>38458.743490</td>\n",
              "      <td>38334.371582</td>\n",
              "      <td>38066.532813</td>\n",
              "      <td>37856.138346</td>\n",
              "      <td>37541.479632</td>\n",
              "      <td>37669.903076</td>\n",
              "      <td>38157.689453</td>\n",
              "      <td>38610.141992</td>\n",
              "      <td>39021.815341</td>\n",
              "      <td>39375.858724</td>\n",
              "      <td>39599.143329</td>\n",
              "      <td>39757.973912</td>\n",
              "      <td>40090.510247</td>\n",
              "      <td>39116.818402</td>\n",
              "      <td>38732.947057</td>\n",
              "      <td>38553.215788</td>\n",
              "      <td>38501.271831</td>\n",
              "      <td>38538.812354</td>\n",
              "      <td>38639.514040</td>\n",
              "      <td>38784.826496</td>\n",
              "      <td>38961.798003</td>\n",
              "      <td>39161.418698</td>\n",
              "      <td>39377.368175</td>\n",
              "      <td>39605.114977</td>\n",
              "      <td>39841.299795</td>\n",
              "      <td>40083.330653</td>\n",
              "      <td>39011.654688</td>\n",
              "      <td>95.684606</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>87.409502</td>\n",
              "      <td>72.867535</td>\n",
              "      <td>82.663595</td>\n",
              "      <td>72.768822</td>\n",
              "      <td>78.830861</td>\n",
              "      <td>76.016493</td>\n",
              "      <td>75.267032</td>\n",
              "      <td>75.915862</td>\n",
              "      <td>71.948634</td>\n",
              "      <td>77.641841</td>\n",
              "      <td>68.921366</td>\n",
              "      <td>70.801283</td>\n",
              "      <td>66.201813</td>\n",
              "      <td>49.300832</td>\n",
              "      <td>63.779583</td>\n",
              "      <td>47.936663</td>\n",
              "      <td>61.630635</td>\n",
              "      <td>45.609687</td>\n",
              "      <td>59.726479</td>\n",
              "      <td>47.228821</td>\n",
              "      <td>58.038989</td>\n",
              "      <td>47.224768</td>\n",
              "      <td>56.542463</td>\n",
              "      <td>49.116906</td>\n",
              "      <td>55.214237</td>\n",
              "      <td>49.879224</td>\n",
              "      <td>45474.297122</td>\n",
              "      <td>33181.179440</td>\n",
              "      <td>42789.021987</td>\n",
              "      <td>34386.845201</td>\n",
              "      <td>41756.869884</td>\n",
              "      <td>35160.617096</td>\n",
              "      <td>41165.917564</td>\n",
              "      <td>35502.825600</td>\n",
              "      <td>40810.522349</td>\n",
              "      <td>35322.543276</td>\n",
              "      <td>40528.704663</td>\n",
              "      <td>35183.572030</td>\n",
              "      <td>40515.002677</td>\n",
              "      <td>34567.956587</td>\n",
              "      <td>40915.430880</td>\n",
              "      <td>34424.375273</td>\n",
              "      <td>42329.370794</td>\n",
              "      <td>33986.008112</td>\n",
              "      <td>43448.225429</td>\n",
              "      <td>33772.058555</td>\n",
              "      <td>44339.727581</td>\n",
              "      <td>33703.903101</td>\n",
              "      <td>45009.360677</td>\n",
              "      <td>33742.356771</td>\n",
              "      <td>45234.138229</td>\n",
              "      <td>33964.148429</td>\n",
              "      <td>45304.487833</td>\n",
              "      <td>34211.459990</td>\n",
              "      <td>-59.710938</td>\n",
              "      <td>-0.143879</td>\n",
              "      <td>hold</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-05</th>\n",
              "      <td>41501.480469</td>\n",
              "      <td>41847.164062</td>\n",
              "      <td>41038.097656</td>\n",
              "      <td>41441.164062</td>\n",
              "      <td>19652846215</td>\n",
              "      <td>25.0</td>\n",
              "      <td>41471.019531</td>\n",
              "      <td>39262.406250</td>\n",
              "      <td>39046.003906</td>\n",
              "      <td>38791.475586</td>\n",
              "      <td>38525.437109</td>\n",
              "      <td>38255.041341</td>\n",
              "      <td>37999.402065</td>\n",
              "      <td>37717.449707</td>\n",
              "      <td>38105.774523</td>\n",
              "      <td>38526.506250</td>\n",
              "      <td>38946.427379</td>\n",
              "      <td>39271.361328</td>\n",
              "      <td>39584.524189</td>\n",
              "      <td>39747.517578</td>\n",
              "      <td>40990.946124</td>\n",
              "      <td>40046.556666</td>\n",
              "      <td>39506.723344</td>\n",
              "      <td>39194.982071</td>\n",
              "      <td>39035.797692</td>\n",
              "      <td>38985.328002</td>\n",
              "      <td>39013.067376</td>\n",
              "      <td>39097.336798</td>\n",
              "      <td>39222.783904</td>\n",
              "      <td>39378.537304</td>\n",
              "      <td>39556.828687</td>\n",
              "      <td>39751.998904</td>\n",
              "      <td>39959.808260</td>\n",
              "      <td>40176.974336</td>\n",
              "      <td>40152.946875</td>\n",
              "      <td>92.121069</td>\n",
              "      <td>98.644776</td>\n",
              "      <td>85.801654</td>\n",
              "      <td>71.084220</td>\n",
              "      <td>81.514834</td>\n",
              "      <td>74.389368</td>\n",
              "      <td>77.920703</td>\n",
              "      <td>73.447278</td>\n",
              "      <td>74.517428</td>\n",
              "      <td>76.282579</td>\n",
              "      <td>71.317541</td>\n",
              "      <td>76.209827</td>\n",
              "      <td>68.381357</td>\n",
              "      <td>79.802026</td>\n",
              "      <td>65.733542</td>\n",
              "      <td>52.318499</td>\n",
              "      <td>63.368888</td>\n",
              "      <td>47.418768</td>\n",
              "      <td>61.266889</td>\n",
              "      <td>45.618786</td>\n",
              "      <td>59.401554</td>\n",
              "      <td>45.687826</td>\n",
              "      <td>57.746555</td>\n",
              "      <td>44.056036</td>\n",
              "      <td>56.277513</td>\n",
              "      <td>49.181683</td>\n",
              "      <td>54.972755</td>\n",
              "      <td>49.376867</td>\n",
              "      <td>41555.463549</td>\n",
              "      <td>41386.575514</td>\n",
              "      <td>44365.868859</td>\n",
              "      <td>34158.943641</td>\n",
              "      <td>43058.962540</td>\n",
              "      <td>35033.045273</td>\n",
              "      <td>42313.565516</td>\n",
              "      <td>35269.385656</td>\n",
              "      <td>41841.977175</td>\n",
              "      <td>35208.897044</td>\n",
              "      <td>41510.289292</td>\n",
              "      <td>34999.793391</td>\n",
              "      <td>41267.030739</td>\n",
              "      <td>34731.773390</td>\n",
              "      <td>41167.028572</td>\n",
              "      <td>34267.870842</td>\n",
              "      <td>42073.410612</td>\n",
              "      <td>34138.138433</td>\n",
              "      <td>43086.622475</td>\n",
              "      <td>33966.390025</td>\n",
              "      <td>44065.394506</td>\n",
              "      <td>33827.460252</td>\n",
              "      <td>44637.484414</td>\n",
              "      <td>33905.238242</td>\n",
              "      <td>45197.465956</td>\n",
              "      <td>33971.582421</td>\n",
              "      <td>45279.664476</td>\n",
              "      <td>34215.370680</td>\n",
              "      <td>971.269531</td>\n",
              "      <td>2.343731</td>\n",
              "      <td>buy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-06</th>\n",
              "      <td>41441.121094</td>\n",
              "      <td>42500.785156</td>\n",
              "      <td>41244.906250</td>\n",
              "      <td>42412.433594</td>\n",
              "      <td>16142097334</td>\n",
              "      <td>25.0</td>\n",
              "      <td>41926.798828</td>\n",
              "      <td>40627.268555</td>\n",
              "      <td>39700.888672</td>\n",
              "      <td>39325.757324</td>\n",
              "      <td>39052.857031</td>\n",
              "      <td>38709.910482</td>\n",
              "      <td>38437.661272</td>\n",
              "      <td>38089.644531</td>\n",
              "      <td>38142.891493</td>\n",
              "      <td>38534.600391</td>\n",
              "      <td>38911.656250</td>\n",
              "      <td>39263.897298</td>\n",
              "      <td>39572.084886</td>\n",
              "      <td>39765.404436</td>\n",
              "      <td>41938.604437</td>\n",
              "      <td>40992.907437</td>\n",
              "      <td>40336.926272</td>\n",
              "      <td>39909.971298</td>\n",
              "      <td>39649.731492</td>\n",
              "      <td>39512.575016</td>\n",
              "      <td>39466.316205</td>\n",
              "      <td>39487.348186</td>\n",
              "      <td>39558.536503</td>\n",
              "      <td>39667.479808</td>\n",
              "      <td>39805.142157</td>\n",
              "      <td>39964.833679</td>\n",
              "      <td>40141.484210</td>\n",
              "      <td>40331.143940</td>\n",
              "      <td>41412.957812</td>\n",
              "      <td>97.203457</td>\n",
              "      <td>94.208335</td>\n",
              "      <td>90.526092</td>\n",
              "      <td>98.929695</td>\n",
              "      <td>85.958511</td>\n",
              "      <td>75.751407</td>\n",
              "      <td>82.215103</td>\n",
              "      <td>75.395399</td>\n",
              "      <td>78.729057</td>\n",
              "      <td>78.008498</td>\n",
              "      <td>75.456115</td>\n",
              "      <td>77.839610</td>\n",
              "      <td>72.434950</td>\n",
              "      <td>79.271500</td>\n",
              "      <td>69.686375</td>\n",
              "      <td>72.635850</td>\n",
              "      <td>67.207732</td>\n",
              "      <td>51.811450</td>\n",
              "      <td>64.982562</td>\n",
              "      <td>50.421623</td>\n",
              "      <td>62.988673</td>\n",
              "      <td>48.099411</td>\n",
              "      <td>61.202815</td>\n",
              "      <td>49.567517</td>\n",
              "      <td>59.602943</td>\n",
              "      <td>49.305396</td>\n",
              "      <td>58.169087</td>\n",
              "      <td>51.031114</td>\n",
              "      <td>43300.381372</td>\n",
              "      <td>40553.216284</td>\n",
              "      <td>45342.045944</td>\n",
              "      <td>35912.491165</td>\n",
              "      <td>44481.898447</td>\n",
              "      <td>34919.878897</td>\n",
              "      <td>43609.269977</td>\n",
              "      <td>35042.244671</td>\n",
              "      <td>43005.433013</td>\n",
              "      <td>35100.281049</td>\n",
              "      <td>42629.516147</td>\n",
              "      <td>34790.304817</td>\n",
              "      <td>42301.499557</td>\n",
              "      <td>34573.822988</td>\n",
              "      <td>42183.931698</td>\n",
              "      <td>33995.357364</td>\n",
              "      <td>42264.213970</td>\n",
              "      <td>34021.569016</td>\n",
              "      <td>43123.035470</td>\n",
              "      <td>33946.165311</td>\n",
              "      <td>43919.375294</td>\n",
              "      <td>33903.937206</td>\n",
              "      <td>44611.209550</td>\n",
              "      <td>33916.585047</td>\n",
              "      <td>45157.338759</td>\n",
              "      <td>33986.831013</td>\n",
              "      <td>45329.721204</td>\n",
              "      <td>34201.087669</td>\n",
              "      <td>1427.851562</td>\n",
              "      <td>3.366587</td>\n",
              "      <td>buy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-07</th>\n",
              "      <td>42406.781250</td>\n",
              "      <td>44401.863281</td>\n",
              "      <td>41748.156250</td>\n",
              "      <td>43840.285156</td>\n",
              "      <td>28641855926</td>\n",
              "      <td>25.0</td>\n",
              "      <td>43126.359375</td>\n",
              "      <td>42298.689453</td>\n",
              "      <td>40550.390625</td>\n",
              "      <td>40066.092773</td>\n",
              "      <td>39658.452344</td>\n",
              "      <td>39292.257487</td>\n",
              "      <td>38950.943917</td>\n",
              "      <td>38640.271729</td>\n",
              "      <td>38318.439670</td>\n",
              "      <td>38607.833008</td>\n",
              "      <td>38944.674716</td>\n",
              "      <td>39294.755046</td>\n",
              "      <td>39567.899639</td>\n",
              "      <td>39837.512416</td>\n",
              "      <td>43206.391583</td>\n",
              "      <td>42131.858525</td>\n",
              "      <td>41337.885954</td>\n",
              "      <td>40783.374378</td>\n",
              "      <td>40411.650340</td>\n",
              "      <td>40178.376576</td>\n",
              "      <td>40049.512066</td>\n",
              "      <td>39999.458417</td>\n",
              "      <td>40009.246888</td>\n",
              "      <td>40064.889841</td>\n",
              "      <td>40156.024157</td>\n",
              "      <td>40274.869797</td>\n",
              "      <td>40415.469466</td>\n",
              "      <td>40573.153679</td>\n",
              "      <td>42698.164453</td>\n",
              "      <td>99.272659</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>94.781008</td>\n",
              "      <td>99.122557</td>\n",
              "      <td>90.606162</td>\n",
              "      <td>78.970131</td>\n",
              "      <td>86.995780</td>\n",
              "      <td>80.774594</td>\n",
              "      <td>83.599405</td>\n",
              "      <td>79.694460</td>\n",
              "      <td>80.375646</td>\n",
              "      <td>81.395287</td>\n",
              "      <td>77.358639</td>\n",
              "      <td>81.159624</td>\n",
              "      <td>74.573249</td>\n",
              "      <td>83.485610</td>\n",
              "      <td>72.024694</td>\n",
              "      <td>58.401612</td>\n",
              "      <td>69.704707</td>\n",
              "      <td>53.572264</td>\n",
              "      <td>67.598210</td>\n",
              "      <td>51.690201</td>\n",
              "      <td>65.687685</td>\n",
              "      <td>51.711944</td>\n",
              "      <td>63.955540</td>\n",
              "      <td>49.768433</td>\n",
              "      <td>62.385205</td>\n",
              "      <td>53.939793</td>\n",
              "      <td>45145.646420</td>\n",
              "      <td>41107.072330</td>\n",
              "      <td>44538.127698</td>\n",
              "      <td>40059.251209</td>\n",
              "      <td>46239.694483</td>\n",
              "      <td>34861.086767</td>\n",
              "      <td>45199.909366</td>\n",
              "      <td>34932.276181</td>\n",
              "      <td>44502.430670</td>\n",
              "      <td>34814.474017</td>\n",
              "      <td>44003.903601</td>\n",
              "      <td>34580.611373</td>\n",
              "      <td>43619.651237</td>\n",
              "      <td>34282.236597</td>\n",
              "      <td>43308.487105</td>\n",
              "      <td>33972.056352</td>\n",
              "      <td>43111.917239</td>\n",
              "      <td>33524.962102</td>\n",
              "      <td>43491.635424</td>\n",
              "      <td>33724.030591</td>\n",
              "      <td>44076.541274</td>\n",
              "      <td>33812.808158</td>\n",
              "      <td>44742.070291</td>\n",
              "      <td>33847.439800</td>\n",
              "      <td>45139.656035</td>\n",
              "      <td>33996.143244</td>\n",
              "      <td>45562.357015</td>\n",
              "      <td>34112.667817</td>\n",
              "      <td>278.160156</td>\n",
              "      <td>0.634485</td>\n",
              "      <td>hold</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 97 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4566aa7b-97df-4778-a033-7e3886195b93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4566aa7b-97df-4778-a033-7e3886195b93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4566aa7b-97df-4778-a033-7e3886195b93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    Open          High           Low         Close  \\\n",
              "Date                                                                 \n",
              "2021-10-31  61850.488281  62406.171875  60074.328125  61318.957031   \n",
              "2021-11-01  61320.449219  62419.003906  59695.183594  61004.406250   \n",
              "2021-11-02  60963.253906  64242.792969  60673.054688  63226.402344   \n",
              "2021-11-03  63254.335938  63516.937500  61184.238281  62970.046875   \n",
              "2021-11-04  62941.804688  63123.289062  60799.664062  61452.230469   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  36944.804688  37154.601562  36375.539062  37154.601562   \n",
              "2022-02-04  37149.265625  41527.785156  37093.628906  41500.875000   \n",
              "2022-02-05  41501.480469  41847.164062  41038.097656  41441.164062   \n",
              "2022-02-06  41441.121094  42500.785156  41244.906250  42412.433594   \n",
              "2022-02-07  42406.781250  44401.863281  41748.156250  43840.285156   \n",
              "\n",
              "                 Volume  value      maclose2      maclose4      maclose6  \\\n",
              "Date                                                                       \n",
              "2021-10-31  32241199927   74.0  61603.894531  61514.472656  60817.345052   \n",
              "2021-11-01  36150572843   74.0  61161.681641  61610.040039  60924.113932   \n",
              "2021-11-02  37746665647   73.0  62115.404297  61859.649414  61714.783203   \n",
              "2021-11-03  36124731509   76.0  63098.224609  62129.953125  62106.101562   \n",
              "2021-11-04  32615846901   73.0  62211.138672  62163.271484  61976.812500   \n",
              "...                 ...    ...           ...           ...           ...   \n",
              "2022-02-03  18591534769   25.0  37053.792969  37833.496094  37898.294271   \n",
              "2022-02-04  29412210792   25.0  39327.738281  38587.933594  38458.743490   \n",
              "2022-02-05  19652846215   25.0  41471.019531  39262.406250  39046.003906   \n",
              "2022-02-06  16142097334   25.0  41926.798828  40627.268555  39700.888672   \n",
              "2022-02-07  28641855926   25.0  43126.359375  42298.689453  40550.390625   \n",
              "\n",
              "                maclose8     maclose10     maclose12     maclose14  \\\n",
              "Date                                                                 \n",
              "2021-10-31  61109.341309  61096.061328  61596.968424  61817.977958   \n",
              "2021-11-01  61118.537598  61127.275391  61181.265951  61745.001395   \n",
              "2021-11-02  61141.859863  61310.553906  61265.951823  61671.030692   \n",
              "2021-11-03  61467.641602  61514.475000  61455.766927  61455.117188   \n",
              "2021-11-04  61838.872070  61355.715625  61460.651367  61400.978516   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  37789.041504  37611.845703  37420.799154  37181.225446   \n",
              "2022-02-04  38334.371582  38066.532813  37856.138346  37541.479632   \n",
              "2022-02-05  38791.475586  38525.437109  38255.041341  37999.402065   \n",
              "2022-02-06  39325.757324  39052.857031  38709.910482  38437.661272   \n",
              "2022-02-07  40066.092773  39658.452344  39292.257487  38950.943917   \n",
              "\n",
              "               maclose16     maclose18     maclose20     maclose22  \\\n",
              "Date                                                                 \n",
              "2021-10-31  61743.593018  61489.608941  61012.755859  60568.703835   \n",
              "2021-11-01  61750.607178  61694.213542  61260.923242  60852.014205   \n",
              "2021-11-02  61855.156250  61784.905382  61552.188477  61112.996626   \n",
              "2021-11-03  61914.154297  61900.342448  61834.614648  61427.950639   \n",
              "2021-11-04  61738.544189  61894.709852  61827.528711  61612.093040   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  37685.118896  38199.338108  38693.968164  39071.392401   \n",
              "2022-02-04  37669.903076  38157.689453  38610.141992  39021.815341   \n",
              "2022-02-05  37717.449707  38105.774523  38526.506250  38946.427379   \n",
              "2022-02-06  38089.644531  38142.891493  38534.600391  38911.656250   \n",
              "2022-02-07  38640.271729  38318.439670  38607.833008  38944.674716   \n",
              "\n",
              "               maclose24     maclose26     maclose28          ema2  \\\n",
              "Date                                                                 \n",
              "2021-10-31  60060.314779  59639.038011  58972.953683  61471.231054   \n",
              "2021-11-01  60353.504720  59856.074820  59397.650251  61160.014518   \n",
              "2021-11-02  60697.595540  60218.398588  59815.921317  62537.606402   \n",
              "2021-11-03  61039.198405  60564.637019  60087.656948  62825.900051   \n",
              "2021-11-04  61204.508464  60814.021935  60360.737165  61910.120329   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  39427.316243  39614.940505  39760.010603  37269.780741   \n",
              "2022-02-04  39375.858724  39599.143329  39757.973912  40090.510247   \n",
              "2022-02-05  39271.361328  39584.524189  39747.517578  40990.946124   \n",
              "2022-02-06  39263.897298  39572.084886  39765.404436  41938.604437   \n",
              "2022-02-07  39294.755046  39567.899639  39837.512416  43206.391583   \n",
              "\n",
              "                    ema4          ema6          ema8         ema10  \\\n",
              "Date                                                                 \n",
              "2021-10-31  61386.530841  61323.459532  61271.284927  61166.798553   \n",
              "2021-11-01  61233.681005  61232.301451  61211.978554  61137.272680   \n",
              "2021-11-02  62030.769540  61802.044563  61659.628285  61517.114437   \n",
              "2021-11-03  62406.480474  62135.759510  61950.832416  61781.283971   \n",
              "2021-11-04  62024.780472  61940.465498  61840.031983  61721.456062   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  37527.447337  37625.775879  37711.027442  37834.693349   \n",
              "2022-02-04  39116.818402  38732.947057  38553.215788  38501.271831   \n",
              "2022-02-05  40046.556666  39506.723344  39194.982071  39035.797692   \n",
              "2022-02-06  40992.907437  40336.926272  39909.971298  39649.731492   \n",
              "2022-02-07  42131.858525  41337.885954  40783.374378  40411.650340   \n",
              "\n",
              "                   ema12         ema14         ema16         ema18  \\\n",
              "Date                                                                 \n",
              "2021-10-31  60990.749539  60749.164121  60457.043581  60130.242404   \n",
              "2021-11-01  60992.850571  60783.196405  60521.439189  60222.259651   \n",
              "2021-11-02  61336.473921  61108.957197  60839.670148  60538.485197   \n",
              "2021-11-03  61587.792837  61357.102487  61090.302704  60794.439058   \n",
              "2021-11-04  61566.937088  61369.786218  61132.882441  60863.680259   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  38000.255509  38199.304662  38422.686695  38663.083062   \n",
              "2022-02-04  38538.812354  38639.514040  38784.826496  38961.798003   \n",
              "2022-02-05  38985.328002  39013.067376  39097.336798  39222.783904   \n",
              "2022-02-06  39512.575016  39466.316205  39487.348186  39558.536503   \n",
              "2022-02-07  40178.376576  40049.512066  39999.458417  40009.246888   \n",
              "\n",
              "                   ema20         ema22         ema24         ema26  \\\n",
              "Date                                                                 \n",
              "2021-10-31  59782.278861  59423.547610  59061.540016  58701.393936   \n",
              "2021-11-01  59898.671945  59561.013579  59216.969315  58871.987441   \n",
              "2021-11-02  60215.598650  59879.743037  59537.723957  59194.536693   \n",
              "2021-11-03  60477.927052  60148.465110  59812.309791  59474.204114   \n",
              "2021-11-04  60570.717854  60261.836011  59943.503445  59620.724585   \n",
              "...                  ...           ...           ...           ...   \n",
              "2022-02-03  38915.160140  39175.129430  39440.266279  39708.533779   \n",
              "2022-02-04  39161.418698  39377.368175  39605.114977  39841.299795   \n",
              "2022-02-05  39378.537304  39556.828687  39751.998904  39959.808260   \n",
              "2022-02-06  39667.479808  39805.142157  39964.833679  40141.484210   \n",
              "2022-02-07  40064.889841  40156.024157  40274.869797  40415.469466   \n",
              "\n",
              "                   ema28           wma   rsiewma2     rsisma2   rsiewma4  \\\n",
              "Date                                                                       \n",
              "2021-10-31  58346.475249  61602.039062  26.649901    0.000000  47.693152   \n",
              "2021-11-01  58529.780836  61398.012500  13.564493    0.000000  39.261251   \n",
              "2021-11-02  58853.685767  62044.557422  92.421634   87.599252  80.289052   \n",
              "2021-11-03  59137.572740  62488.716406  70.241538   89.656210  71.059463   \n",
              "2021-11-04  59297.204308  62217.627344  13.346981    0.000000  33.293079   \n",
              "...                  ...           ...        ...         ...        ...   \n",
              "2022-02-03  39978.327368  37544.703125  29.757072   10.121821  30.670476   \n",
              "2022-02-04  40083.330653  39011.654688  95.684606  100.000000  87.409502   \n",
              "2022-02-05  40176.974336  40152.946875  92.121069   98.644776  85.801654   \n",
              "2022-02-06  40331.143940  41412.957812  97.203457   94.208335  90.526092   \n",
              "2022-02-07  40573.153679  42698.164453  99.272659  100.000000  94.781008   \n",
              "\n",
              "              rsisma4   rsiewma6    rsisma6   rsiewma8    rsisma8  rsiewma10  \\\n",
              "Date                                                                           \n",
              "2021-10-31  80.470705  49.923215  40.659668  50.529448  49.683208  51.247001   \n",
              "2021-11-01  56.755343  45.086107  54.675639  47.229602  50.316144  48.734381   \n",
              "2021-11-02  64.488777  71.957194  82.985176  66.876168  50.794046  63.981473   \n",
              "2021-11-03  66.076218  66.686495  72.117803  63.376183  63.968718  61.406204   \n",
              "2021-11-04  51.545838  41.493537  42.569205  45.320600  66.562983  47.554906   \n",
              "...               ...        ...        ...        ...        ...        ...   \n",
              "2022-02-03  36.460003  35.141693  40.717427  37.503237  53.497519  38.308944   \n",
              "2022-02-04  72.867535  82.663595  72.768822  78.830861  76.016493  75.267032   \n",
              "2022-02-05  71.084220  81.514834  74.389368  77.920703  73.447278  74.517428   \n",
              "2022-02-06  98.929695  85.958511  75.751407  82.215103  75.395399  78.729057   \n",
              "2022-02-07  99.122557  90.606162  78.970131  86.995780  80.774594  83.599405   \n",
              "\n",
              "             rsisma10  rsiewma12   rsisma12  rsiewma14   rsisma14  rsiewma16  \\\n",
              "Date                                                                           \n",
              "2021-10-31  46.817783  52.130428  42.460159  53.063933  49.472079  53.959510   \n",
              "2021-11-01  51.219328  50.085169  36.220010  51.327596  47.685071  52.443879   \n",
              "2021-11-02  56.399238  62.400419  53.072114  61.575650  47.652054  61.174622   \n",
              "2021-11-03  57.224116  60.369551  57.454426  59.896809  42.655510  59.740550   \n",
              "2021-11-04  44.129912  49.171661  50.182091  50.492043  47.930679  51.620541   \n",
              "...               ...        ...        ...        ...        ...        ...   \n",
              "2022-02-03  55.293032  38.346414  66.727569  38.069199  35.309122  37.696246   \n",
              "2022-02-04  75.915862  71.948634  77.641841  68.921366  70.801283  66.201813   \n",
              "2022-02-05  76.282579  71.317541  76.209827  68.381357  79.802026  65.733542   \n",
              "2022-02-06  78.008498  75.456115  77.839610  72.434950  79.271500  69.686375   \n",
              "2022-02-07  79.694460  80.375646  81.395287  77.358639  81.159624  74.573249   \n",
              "\n",
              "             rsisma16  rsiewma18   rsisma18  rsiewma20   rsisma20  rsiewma22  \\\n",
              "Date                                                                           \n",
              "2021-10-31  49.417097  54.770702  57.011166  55.479988  56.235654  56.086492   \n",
              "2021-11-01  50.241858  53.421911  56.535701  54.262630  58.379863  54.975894   \n",
              "2021-11-02  53.377797  61.003780  53.124350  60.952180  59.556985  60.957257   \n",
              "2021-11-03  51.922906  59.749675  54.045806  59.836204  59.213514  59.950885   \n",
              "2021-11-04  44.103897  52.594807  49.808961  53.434052  49.746013  54.153347   \n",
              "...               ...        ...        ...        ...        ...        ...   \n",
              "2022-02-03  30.938511  37.325422  29.707772  36.996761  29.948896  36.723595   \n",
              "2022-02-04  49.300832  63.779583  47.936663  61.630635  45.609687  59.726479   \n",
              "2022-02-05  52.318499  63.368888  47.418768  61.266889  45.618786  59.401554   \n",
              "2022-02-06  72.635850  67.207732  51.811450  64.982562  50.421623  62.988673   \n",
              "2022-02-07  83.485610  72.024694  58.401612  69.704707  53.572264  67.598210   \n",
              "\n",
              "             rsisma22  rsiewma24   rsisma24  rsiewma26   rsisma26  rsiewma28  \\\n",
              "Date                                                                           \n",
              "2021-10-31  59.435399  56.597705  60.789539  57.024544  62.188720  57.378653   \n",
              "2021-11-01  59.227894  55.575853  60.061227  56.077800  57.690882  56.496387   \n",
              "2021-11-02  58.626103  60.984526  61.409379  61.015645  62.610140  61.041437   \n",
              "2021-11-03  60.795143  60.067415  61.308225  60.172745  62.019907  60.261274   \n",
              "2021-11-04  56.280659  54.766996  55.658941  55.288780  58.539570  55.731544   \n",
              "...               ...        ...        ...        ...        ...        ...   \n",
              "2022-02-03  29.645635  36.507402  37.600781  36.344774  38.003634  36.230610   \n",
              "2022-02-04  47.228821  58.038989  47.224768  56.542463  49.116906  55.214237   \n",
              "2022-02-05  45.687826  57.746555  44.056036  56.277513  49.181683  54.972755   \n",
              "2022-02-06  48.099411  61.202815  49.567517  59.602943  49.305396  58.169087   \n",
              "2022-02-07  51.690201  65.687685  51.711944  63.955540  49.768433  62.385205   \n",
              "\n",
              "             rsisma28  bollingerupper2  bollingerlower2  bollingerupper4  \\\n",
              "Date                                                                       \n",
              "2021-10-31  65.067896     62409.819485     60797.969577     62920.984099   \n",
              "2021-11-01  63.848403     61606.523622     60716.839660     62712.206175   \n",
              "2021-11-02  63.696275     65257.781308     58973.027286     63823.541576   \n",
              "2021-11-03  59.713673     63460.765990     62735.683229     64390.492538   \n",
              "2021-11-04  59.771130     64357.655219     60064.622125     64363.175031   \n",
              "...               ...              ...              ...              ...   \n",
              "2022-02-03  35.606987     37338.922730     36768.663208     39654.086182   \n",
              "2022-02-04  49.879224     45474.297122     33181.179440     42789.021987   \n",
              "2022-02-05  49.376867     41555.463549     41386.575514     44365.868859   \n",
              "2022-02-06  51.031114     43300.381372     40553.216284     45342.045944   \n",
              "2022-02-07  53.939793     45145.646420     41107.072330     44538.127698   \n",
              "\n",
              "            bollingerlower4  bollingerupper6  bollingerlower6  \\\n",
              "Date                                                            \n",
              "2021-10-31     60107.961214     63513.326257     58121.363847   \n",
              "2021-11-01     60507.873903     63584.381312     58263.846553   \n",
              "2021-11-02     59895.757252     63598.148474     59831.417932   \n",
              "2021-11-03     59869.413712     63871.742778     60340.460347   \n",
              "2021-11-04     59963.367937     63811.863214     60141.761786   \n",
              "...                     ...              ...              ...   \n",
              "2022-02-03     36012.906005     39329.552236     36467.036306   \n",
              "2022-02-04     34386.845201     41756.869884     35160.617096   \n",
              "2022-02-05     34158.943641     43058.962540     35033.045273   \n",
              "2022-02-06     35912.491165     44481.898447     34919.878897   \n",
              "2022-02-07     40059.251209     46239.694483     34861.086767   \n",
              "\n",
              "            bollingerupper8  bollingerlower8  bollingerupper10  \\\n",
              "Date                                                             \n",
              "2021-10-31     63871.906642     58346.775975      63555.383183   \n",
              "2021-11-01     63878.875389     58358.199806      63571.697656   \n",
              "2021-11-02     63978.504093     58305.215634      64094.929866   \n",
              "2021-11-03     64488.452569     58446.830634      64468.762234   \n",
              "2021-11-04     63683.596963     59994.147177      64109.518502   \n",
              "...                     ...              ...               ...   \n",
              "2022-02-03     39110.472179     36467.610828      38996.995102   \n",
              "2022-02-04     41165.917564     35502.825600      40810.522349   \n",
              "2022-02-05     42313.565516     35269.385656      41841.977175   \n",
              "2022-02-06     43609.269977     35042.244671      43005.433013   \n",
              "2022-02-07     45199.909366     34932.276181      44502.430670   \n",
              "\n",
              "            bollingerlower10  bollingerupper12  bollingerlower12  \\\n",
              "Date                                                               \n",
              "2021-10-31      58636.739474      65205.914018      57988.022831   \n",
              "2021-11-01      58682.853126      63498.875926      58863.655975   \n",
              "2021-11-02      58526.177947      63810.750018      58721.153628   \n",
              "2021-11-03      58560.187766      64149.291884      58762.241970   \n",
              "2021-11-04      58601.912748      64153.897096      58767.405638   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      36226.696304      38967.423213      35874.175094   \n",
              "2022-02-04      35322.543276      40528.704663      35183.572030   \n",
              "2022-02-05      35208.897044      41510.289292      34999.793391   \n",
              "2022-02-06      35100.281049      42629.516147      34790.304817   \n",
              "2022-02-07      34814.474017      44003.903601      34580.611373   \n",
              "\n",
              "            bollingerupper14  bollingerlower14  bollingerupper16  \\\n",
              "Date                                                               \n",
              "2021-10-31      65430.775579      58205.180336      65140.001150   \n",
              "2021-11-01      65380.892219      58109.110572      65139.969399   \n",
              "2021-11-02      65123.866074      58218.195310      65320.929437   \n",
              "2021-11-03      64003.330200      58906.904175      65424.197552   \n",
              "2021-11-04      63912.023831      58889.933200      65021.190358   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      39135.913133      35226.537759      41008.624254   \n",
              "2022-02-04      40515.002677      34567.956587      40915.430880   \n",
              "2022-02-05      41267.030739      34731.773390      41167.028572   \n",
              "2022-02-06      42301.499557      34573.822988      42183.931698   \n",
              "2022-02-07      43619.651237      34282.236597      43308.487105   \n",
              "\n",
              "            bollingerlower16  bollingerupper18  bollingerlower18  \\\n",
              "Date                                                               \n",
              "2021-10-31      58347.184885      65299.021618      57680.196264   \n",
              "2021-11-01      58361.244956      64903.881148      58484.545935   \n",
              "2021-11-02      58389.383063      65073.847838      58495.962926   \n",
              "2021-11-03      58404.111042      65202.412880      58598.272016   \n",
              "2021-11-04      58455.898021      65199.629846      58589.789859   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      34361.613539      42524.526534      33874.149682   \n",
              "2022-02-04      34424.375273      42329.370794      33986.008112   \n",
              "2022-02-05      34267.870842      42073.410612      34138.138433   \n",
              "2022-02-06      33995.357364      42264.213970      34021.569016   \n",
              "2022-02-07      33972.056352      43111.917239      33524.962102   \n",
              "\n",
              "            bollingerupper20  bollingerlower20  bollingerupper22  \\\n",
              "Date                                                               \n",
              "2021-10-31      65681.335136      56344.176583      65924.432322   \n",
              "2021-11-01      65302.283336      57219.563148      65540.550178   \n",
              "2021-11-02      65247.076980      57857.299973      65652.946154   \n",
              "2021-11-03      64992.377345      58676.851952      65422.000060   \n",
              "2021-11-04      64988.199976      58666.857446      65178.852829   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      43793.946522      33593.989806      44505.225405   \n",
              "2022-02-04      43448.225429      33772.058555      44339.727581   \n",
              "2022-02-05      43086.622475      33966.390025      44065.394506   \n",
              "2022-02-06      43123.035470      33946.165311      43919.375294   \n",
              "2022-02-07      43491.635424      33724.030591      44076.541274   \n",
              "\n",
              "            bollingerlower22  bollingerupper24  bollingerlower24  \\\n",
              "Date                                                               \n",
              "2021-10-31      55212.975349      66236.337987      53884.291570   \n",
              "2021-11-01      56163.478231      65964.578635      54742.430806   \n",
              "2021-11-02      56573.047099      65930.344909      55464.846172   \n",
              "2021-11-03      57433.901219      65695.943144      56382.453666   \n",
              "2021-11-04      58045.333251      65609.470552      56799.546375   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      33637.559396      45163.461205      33691.171282   \n",
              "2022-02-04      33703.903101      45009.360677      33742.356771   \n",
              "2022-02-05      33827.460252      44637.484414      33905.238242   \n",
              "2022-02-06      33903.937206      44611.209550      33916.585047   \n",
              "2022-02-07      33812.808158      44742.070291      33847.439800   \n",
              "\n",
              "            bollingerupper26  bollingerlower26  bollingerupper28  \\\n",
              "Date                                                               \n",
              "2021-10-31      66283.198391      52994.877631      67049.304882   \n",
              "2021-11-01      66284.102883      53428.046757      66517.171509   \n",
              "2021-11-02      66279.282045      54157.515130      66367.853769   \n",
              "2021-11-03      66149.970682      54979.303356      66502.933121   \n",
              "2021-11-04      65918.148195      55709.895676      66300.135421   \n",
              "...                      ...               ...               ...   \n",
              "2022-02-03      45274.363633      33955.517376      45309.220551   \n",
              "2022-02-04      45234.138229      33964.148429      45304.487833   \n",
              "2022-02-05      45197.465956      33971.582421      45279.664476   \n",
              "2022-02-06      45157.338759      33986.831013      45329.721204   \n",
              "2022-02-07      45139.656035      33996.143244      45562.357015   \n",
              "\n",
              "            bollingerlower28         diff  percen_diff target  buy  hold  sell  \n",
              "Date                                                                            \n",
              "2021-10-31      50896.602484  -314.550781    -0.512975   hold    0     1     0  \n",
              "2021-11-01      52278.128993  2221.996094     3.642353    buy    1     0     0  \n",
              "2021-11-02      53263.988865  -256.355469    -0.405456   hold    0     1     0  \n",
              "2021-11-03      53672.380774 -1517.816406    -2.410378   sell    0     0     1  \n",
              "2021-11-04      54421.338909  -326.554688    -0.531396   hold    0     1     0  \n",
              "...                      ...          ...          ...    ...  ...   ...   ...  \n",
              "2022-02-03      34210.800654  4346.273438    11.697807    buy    1     0     0  \n",
              "2022-02-04      34211.459990   -59.710938    -0.143879   hold    0     1     0  \n",
              "2022-02-05      34215.370680   971.269531     2.343731    buy    1     0     0  \n",
              "2022-02-06      34201.087669  1427.851562     3.366587    buy    1     0     0  \n",
              "2022-02-07      34112.667817   278.160156     0.634485   hold    0     1     0  \n",
              "\n",
              "[100 rows x 97 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rs = df.drop(['diff', 'percen_diff', 'target'], axis=1)\n",
        "df_rs = np.array(df_rs)"
      ],
      "metadata": {
        "id": "oU-aNvJKlGOA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence(sequence, n_steps_in):\n",
        "    X, y = list(), list()\n",
        "    range = np.arange(start = 0, stop = len(sequence), step = 1) # Ojo con el step que te fundio :)\n",
        "    for i in range:\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x = sequence[i:end_ix, :-3]\n",
        "        seq_y = sequence[i:end_ix, -3:][-1]\n",
        "        if len(seq_x) == n_steps_in:\n",
        "          X.append(seq_x)\n",
        "          y.append(seq_y)\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "1uS4FwZFmrDY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps_in = past_t\n",
        "X, y = split_sequence(df_rs, n_steps_in)"
      ],
      "metadata": {
        "id": "_zCWN23wnNRA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 200\n",
        "train_size = len(X) - test_size\n",
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(X)]\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1,1)) #Try this\n",
        "#scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "X_new = scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "# save the scaler\n",
        "dump(scaler, open(folder + 'scaler.pkl', 'wb'))\n",
        "\n",
        "print(\"Shape X_train --> \" + str(X_train.shape))\n",
        "print(\"Shape y_train --> \" + str(y_train.shape))\n",
        "print(\"Shape X_test  --> \" + str(X_test.shape))\n",
        "print(\"Shape y_test  --> \" + str(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLAC3khUh2sZ",
        "outputId": "d75aa133-f907-49be-cb3e-f4c6fc13eaa1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape X_train --> (1234, 7, 91)\n",
            "Shape y_train --> (1234, 3)\n",
            "Shape X_test  --> (200, 7, 91)\n",
            "Shape y_test  --> (200, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "W4ktG3Uyrdkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations"
      ],
      "metadata": {
        "id": "JbC-f5QaiLpr"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get square shape\n",
        "image_shape = X_train.shape[1]\n",
        "\n",
        "if X_train.shape[1]<X_train.shape[2]:\n",
        "  image_shape = X_train.shape[2]\n",
        "\n",
        "# Load Resnet model\n",
        "base_model = tf.keras.applications.ResNet152V2(\n",
        "    weights='imagenet',\n",
        "    input_shape=(image_shape, image_shape, 3),\n",
        "    include_top=False) \n",
        "base_model.trainable = False\n",
        "\n",
        "# Create input as image\n",
        "inputs = layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "if X_train.shape[1]<X_train.shape[2]:\n",
        "  input_img = layers.Reshape((X_train.shape[2], X_train.shape[1]))(inputs) \n",
        "  input_img = layers.Dense(image_shape, activation='tanh')(input_img) # Create a square matrix \n",
        "  input_img = layers.Reshape((image_shape, image_shape, 1))(input_img) # Reshape into an image with 1 channels\n",
        "  input_img = layers.Dense(3, activation='tanh', name='image_inp')(input_img) # Learn 3 channels\n",
        "else:\n",
        "  input_img = layers.Dense(image_shape, activation='tanh')(inputs) # Create a square matrix \n",
        "  input_img = layers.Reshape((image_shape, image_shape, 1))(input_img) # Reshape into an image with 1 channels\n",
        "  input_img = layers.Dense(3, activation='tanh', name='image_inp')(input_img) # Learn 3 channels\n",
        "# With tahn all outputs are between -1 and 1 (ResNet needs that)\n",
        "\n",
        "# Create Model\n",
        "x = base_model(input_img, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "#x = layers.Dropout(0.2)(x) \n",
        "\n",
        "# Output layer\n",
        "predictions = layers.Dense(y_train.shape[1], activation=\"softmax\")(x) #Relu for getting positive values\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics='accuracy')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0whdy_frnFm",
        "outputId": "dc889cdc-1590-485a-fe39-b7840de76c58"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 7, 91)]           0         \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 91, 7)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 91, 91)            728       \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 91, 91, 1)         0         \n",
            "                                                                 \n",
            " image_inp (Dense)           (None, 91, 91, 3)         6         \n",
            "                                                                 \n",
            " resnet152v2 (Functional)    (None, 3, 3, 2048)        58331648  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,338,529\n",
            "Trainable params: 6,881\n",
            "Non-trainable params: 58,331,648\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, mode='auto')\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='loss', patience=7, verbose=1, factor=0.5, min_lr=5e-5)"
      ],
      "metadata": {
        "id": "NvJEXVyds0IE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs = 700, batch_size = 16, \n",
        "                    callbacks=[learning_rate_reduction, early_stopping],\n",
        "                    validation_data= (X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2oV4Ss9hs5nE",
        "outputId": "d9719f79-ff6f-4c73-d819-3729733d21a7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "78/78 [==============================] - 20s 142ms/step - loss: 1.1552 - accuracy: 0.3784 - val_loss: 1.1701 - val_accuracy: 0.3200 - lr: 0.0100\n",
            "Epoch 2/700\n",
            "78/78 [==============================] - 7s 94ms/step - loss: 1.1086 - accuracy: 0.3793 - val_loss: 1.1152 - val_accuracy: 0.3350 - lr: 0.0100\n",
            "Epoch 3/700\n",
            "78/78 [==============================] - 7s 94ms/step - loss: 1.0943 - accuracy: 0.3857 - val_loss: 1.1840 - val_accuracy: 0.3500 - lr: 0.0100\n",
            "Epoch 4/700\n",
            "78/78 [==============================] - 7s 94ms/step - loss: 1.1007 - accuracy: 0.4052 - val_loss: 1.1724 - val_accuracy: 0.3700 - lr: 0.0100\n",
            "Epoch 5/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0948 - accuracy: 0.3906 - val_loss: 1.1176 - val_accuracy: 0.3750 - lr: 0.0100\n",
            "Epoch 6/700\n",
            "78/78 [==============================] - 7s 95ms/step - loss: 1.0876 - accuracy: 0.4100 - val_loss: 1.1556 - val_accuracy: 0.3550 - lr: 0.0100\n",
            "Epoch 7/700\n",
            "78/78 [==============================] - 8s 96ms/step - loss: 1.0675 - accuracy: 0.4352 - val_loss: 1.1744 - val_accuracy: 0.3600 - lr: 0.0100\n",
            "Epoch 8/700\n",
            "78/78 [==============================] - 8s 104ms/step - loss: 1.0916 - accuracy: 0.4044 - val_loss: 1.3260 - val_accuracy: 0.3750 - lr: 0.0100\n",
            "Epoch 9/700\n",
            "78/78 [==============================] - 7s 95ms/step - loss: 1.0927 - accuracy: 0.4052 - val_loss: 1.2274 - val_accuracy: 0.3450 - lr: 0.0100\n",
            "Epoch 10/700\n",
            "78/78 [==============================] - 7s 95ms/step - loss: 1.0793 - accuracy: 0.4198 - val_loss: 1.1791 - val_accuracy: 0.3650 - lr: 0.0100\n",
            "Epoch 11/700\n",
            "78/78 [==============================] - 7s 95ms/step - loss: 1.0753 - accuracy: 0.4230 - val_loss: 1.2626 - val_accuracy: 0.2900 - lr: 0.0100\n",
            "Epoch 12/700\n",
            "78/78 [==============================] - 7s 96ms/step - loss: 1.0644 - accuracy: 0.4376 - val_loss: 1.2253 - val_accuracy: 0.3250 - lr: 0.0100\n",
            "Epoch 13/700\n",
            "78/78 [==============================] - 7s 95ms/step - loss: 1.0721 - accuracy: 0.4384 - val_loss: 1.1992 - val_accuracy: 0.3650 - lr: 0.0100\n",
            "Epoch 14/700\n",
            "78/78 [==============================] - 7s 96ms/step - loss: 1.0923 - accuracy: 0.4165 - val_loss: 1.1717 - val_accuracy: 0.3750 - lr: 0.0100\n",
            "Epoch 15/700\n",
            "78/78 [==============================] - 7s 96ms/step - loss: 1.0632 - accuracy: 0.4408 - val_loss: 1.3103 - val_accuracy: 0.3200 - lr: 0.0100\n",
            "Epoch 16/700\n",
            "78/78 [==============================] - 8s 96ms/step - loss: 1.0559 - accuracy: 0.4449 - val_loss: 1.2137 - val_accuracy: 0.3350 - lr: 0.0100\n",
            "Epoch 17/700\n",
            "78/78 [==============================] - 7s 96ms/step - loss: 1.0720 - accuracy: 0.4238 - val_loss: 1.2698 - val_accuracy: 0.3250 - lr: 0.0100\n",
            "Epoch 18/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 1.0633 - accuracy: 0.4417 - val_loss: 1.1864 - val_accuracy: 0.3550 - lr: 0.0100\n",
            "Epoch 19/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0528 - accuracy: 0.4433 - val_loss: 1.4066 - val_accuracy: 0.3050 - lr: 0.0100\n",
            "Epoch 20/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0688 - accuracy: 0.4295 - val_loss: 1.2585 - val_accuracy: 0.3400 - lr: 0.0100\n",
            "Epoch 21/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0534 - accuracy: 0.4400 - val_loss: 1.3341 - val_accuracy: 0.2900 - lr: 0.0100\n",
            "Epoch 22/700\n",
            "78/78 [==============================] - 8s 108ms/step - loss: 1.0373 - accuracy: 0.4635 - val_loss: 1.2615 - val_accuracy: 0.3500 - lr: 0.0100\n",
            "Epoch 23/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0639 - accuracy: 0.4449 - val_loss: 1.1969 - val_accuracy: 0.3600 - lr: 0.0100\n",
            "Epoch 24/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0617 - accuracy: 0.4400 - val_loss: 1.3977 - val_accuracy: 0.3100 - lr: 0.0100\n",
            "Epoch 25/700\n",
            "78/78 [==============================] - 8s 96ms/step - loss: 1.0502 - accuracy: 0.4344 - val_loss: 1.4349 - val_accuracy: 0.3550 - lr: 0.0100\n",
            "Epoch 26/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0473 - accuracy: 0.4611 - val_loss: 1.2014 - val_accuracy: 0.3600 - lr: 0.0100\n",
            "Epoch 27/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0432 - accuracy: 0.4643 - val_loss: 1.3309 - val_accuracy: 0.3200 - lr: 0.0100\n",
            "Epoch 28/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 1.0648 - accuracy: 0.4287 - val_loss: 1.3973 - val_accuracy: 0.3500 - lr: 0.0100\n",
            "Epoch 29/700\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0392 - accuracy: 0.4587\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0392 - accuracy: 0.4587 - val_loss: 1.2414 - val_accuracy: 0.3550 - lr: 0.0100\n",
            "Epoch 30/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 1.0098 - accuracy: 0.4789 - val_loss: 1.3470 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 31/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9949 - accuracy: 0.5000 - val_loss: 1.4036 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 32/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9893 - accuracy: 0.5024 - val_loss: 1.2972 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 33/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9824 - accuracy: 0.5041 - val_loss: 1.3228 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 34/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9795 - accuracy: 0.5089 - val_loss: 1.3500 - val_accuracy: 0.3300 - lr: 0.0050\n",
            "Epoch 35/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9620 - accuracy: 0.5462 - val_loss: 1.3984 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 36/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.9618 - accuracy: 0.5259 - val_loss: 1.4267 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 37/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9576 - accuracy: 0.5300 - val_loss: 1.4249 - val_accuracy: 0.3250 - lr: 0.0050\n",
            "Epoch 38/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9565 - accuracy: 0.5373 - val_loss: 1.4001 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 39/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9496 - accuracy: 0.5421 - val_loss: 1.3994 - val_accuracy: 0.3250 - lr: 0.0050\n",
            "Epoch 40/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.9509 - accuracy: 0.5365 - val_loss: 1.3695 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 41/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9451 - accuracy: 0.5527 - val_loss: 1.5091 - val_accuracy: 0.3150 - lr: 0.0050\n",
            "Epoch 42/700\n",
            "78/78 [==============================] - 8s 97ms/step - loss: 0.9357 - accuracy: 0.5543 - val_loss: 1.4479 - val_accuracy: 0.3300 - lr: 0.0050\n",
            "Epoch 43/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9293 - accuracy: 0.5502 - val_loss: 1.4686 - val_accuracy: 0.3200 - lr: 0.0050\n",
            "Epoch 44/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9173 - accuracy: 0.5567 - val_loss: 1.4614 - val_accuracy: 0.3050 - lr: 0.0050\n",
            "Epoch 45/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9078 - accuracy: 0.5810 - val_loss: 1.4079 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 46/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.9089 - accuracy: 0.5648 - val_loss: 1.5042 - val_accuracy: 0.3250 - lr: 0.0050\n",
            "Epoch 47/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9121 - accuracy: 0.5737 - val_loss: 1.5328 - val_accuracy: 0.3300 - lr: 0.0050\n",
            "Epoch 48/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.9072 - accuracy: 0.5648 - val_loss: 1.5195 - val_accuracy: 0.3300 - lr: 0.0050\n",
            "Epoch 49/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8986 - accuracy: 0.5600 - val_loss: 1.5099 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 50/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.9024 - accuracy: 0.5689 - val_loss: 1.5754 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 51/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8934 - accuracy: 0.5746 - val_loss: 1.4426 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 52/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8966 - accuracy: 0.5632 - val_loss: 1.6353 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 53/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.8910 - accuracy: 0.5827 - val_loss: 1.5841 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 54/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8937 - accuracy: 0.5705 - val_loss: 1.5210 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 55/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8937 - accuracy: 0.5770 - val_loss: 1.6039 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 56/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8714 - accuracy: 0.5900 - val_loss: 1.5943 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 57/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8771 - accuracy: 0.5770 - val_loss: 1.6592 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 58/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8643 - accuracy: 0.5859 - val_loss: 1.6141 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 59/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8557 - accuracy: 0.6005 - val_loss: 1.6482 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 60/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8505 - accuracy: 0.6029 - val_loss: 1.7326 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 61/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.8416 - accuracy: 0.6021 - val_loss: 1.6532 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 62/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.8386 - accuracy: 0.6094 - val_loss: 1.6591 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 63/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8302 - accuracy: 0.6183 - val_loss: 1.6512 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 64/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8365 - accuracy: 0.6005 - val_loss: 1.7180 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 65/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8276 - accuracy: 0.6199 - val_loss: 1.6874 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 66/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8292 - accuracy: 0.6118 - val_loss: 1.7616 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 67/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.8145 - accuracy: 0.6167 - val_loss: 1.7722 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 68/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.8244 - accuracy: 0.6110 - val_loss: 1.7598 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 69/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8134 - accuracy: 0.6224 - val_loss: 1.7689 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 70/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.8009 - accuracy: 0.6272 - val_loss: 1.7762 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 71/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8202 - accuracy: 0.6013 - val_loss: 1.8833 - val_accuracy: 0.3250 - lr: 0.0050\n",
            "Epoch 72/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8181 - accuracy: 0.6224 - val_loss: 1.8465 - val_accuracy: 0.3300 - lr: 0.0050\n",
            "Epoch 73/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7980 - accuracy: 0.6248 - val_loss: 1.8850 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 74/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7895 - accuracy: 0.6280 - val_loss: 1.8503 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 75/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7904 - accuracy: 0.6434 - val_loss: 1.8676 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 76/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.8021 - accuracy: 0.6240 - val_loss: 1.8674 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 77/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7948 - accuracy: 0.6288 - val_loss: 1.7631 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 78/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7967 - accuracy: 0.6345 - val_loss: 1.9280 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 79/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7998 - accuracy: 0.6313 - val_loss: 1.9513 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 80/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7717 - accuracy: 0.6459 - val_loss: 1.8118 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 81/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7719 - accuracy: 0.6361 - val_loss: 1.9734 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 82/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7722 - accuracy: 0.6515 - val_loss: 1.9129 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 83/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7772 - accuracy: 0.6410 - val_loss: 1.8301 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 84/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7579 - accuracy: 0.6564 - val_loss: 1.8154 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 85/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7728 - accuracy: 0.6572 - val_loss: 1.8766 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 86/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7584 - accuracy: 0.6451 - val_loss: 1.9437 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 87/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7571 - accuracy: 0.6491 - val_loss: 2.0228 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 88/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7544 - accuracy: 0.6572 - val_loss: 1.8967 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 89/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7485 - accuracy: 0.6621 - val_loss: 2.0201 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 90/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7381 - accuracy: 0.6629 - val_loss: 1.9340 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 91/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7497 - accuracy: 0.6653 - val_loss: 1.9531 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 92/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7399 - accuracy: 0.6637 - val_loss: 2.0308 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 93/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7499 - accuracy: 0.6580 - val_loss: 1.9996 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 94/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7467 - accuracy: 0.6629 - val_loss: 2.0706 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 95/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7327 - accuracy: 0.6702 - val_loss: 2.0608 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 96/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7418 - accuracy: 0.6645 - val_loss: 2.0529 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 97/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.7300 - accuracy: 0.6823 - val_loss: 2.0106 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 98/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.7197 - accuracy: 0.6799 - val_loss: 2.0965 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 99/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7185 - accuracy: 0.6904 - val_loss: 2.1878 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 100/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7190 - accuracy: 0.6872 - val_loss: 2.1592 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 101/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7158 - accuracy: 0.6831 - val_loss: 2.0806 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 102/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.7094 - accuracy: 0.6864 - val_loss: 2.2930 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 103/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7179 - accuracy: 0.6775 - val_loss: 2.1998 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 104/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7124 - accuracy: 0.6872 - val_loss: 2.2499 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 105/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.7115 - accuracy: 0.6864 - val_loss: 2.1454 - val_accuracy: 0.3900 - lr: 0.0050\n",
            "Epoch 106/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6965 - accuracy: 0.6929 - val_loss: 2.1914 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 107/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7226 - accuracy: 0.6718 - val_loss: 2.2336 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 108/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6983 - accuracy: 0.7010 - val_loss: 2.2884 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 109/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6968 - accuracy: 0.7034 - val_loss: 2.2922 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 110/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6917 - accuracy: 0.6969 - val_loss: 2.2680 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 111/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6778 - accuracy: 0.7075 - val_loss: 2.1758 - val_accuracy: 0.3900 - lr: 0.0050\n",
            "Epoch 112/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6935 - accuracy: 0.7010 - val_loss: 2.2823 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 113/700\n",
            "78/78 [==============================] - 8s 101ms/step - loss: 0.6838 - accuracy: 0.6929 - val_loss: 2.4176 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 114/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6890 - accuracy: 0.6937 - val_loss: 2.3457 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 115/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6719 - accuracy: 0.7139 - val_loss: 2.3195 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 116/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6798 - accuracy: 0.7050 - val_loss: 2.4658 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 117/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6732 - accuracy: 0.7002 - val_loss: 2.2823 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 118/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.7130 - accuracy: 0.6759 - val_loss: 2.5758 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 119/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6868 - accuracy: 0.6896 - val_loss: 2.4129 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 120/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6725 - accuracy: 0.6977 - val_loss: 2.4998 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 121/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6637 - accuracy: 0.7204 - val_loss: 2.4412 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 122/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6639 - accuracy: 0.7099 - val_loss: 2.3558 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 123/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6596 - accuracy: 0.7123 - val_loss: 2.4477 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 124/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6743 - accuracy: 0.7018 - val_loss: 2.4063 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 125/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6541 - accuracy: 0.7261 - val_loss: 2.4782 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 126/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6644 - accuracy: 0.7042 - val_loss: 2.5792 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 127/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6587 - accuracy: 0.7139 - val_loss: 2.3478 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 128/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6517 - accuracy: 0.7139 - val_loss: 2.4976 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 129/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6417 - accuracy: 0.7277 - val_loss: 2.4873 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 130/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6553 - accuracy: 0.7196 - val_loss: 2.4718 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 131/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6671 - accuracy: 0.7083 - val_loss: 2.5282 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 132/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6449 - accuracy: 0.7091 - val_loss: 2.4858 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 133/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6440 - accuracy: 0.7229 - val_loss: 2.6531 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 134/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6445 - accuracy: 0.7066 - val_loss: 2.7254 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 135/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6461 - accuracy: 0.7326 - val_loss: 2.5276 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 136/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6341 - accuracy: 0.7245 - val_loss: 2.7346 - val_accuracy: 0.3950 - lr: 0.0050\n",
            "Epoch 137/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6466 - accuracy: 0.7180 - val_loss: 2.7450 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 138/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6238 - accuracy: 0.7374 - val_loss: 2.5667 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 139/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6326 - accuracy: 0.7358 - val_loss: 2.5013 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 140/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6385 - accuracy: 0.7147 - val_loss: 2.5846 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 141/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6219 - accuracy: 0.7350 - val_loss: 2.7010 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 142/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6242 - accuracy: 0.7342 - val_loss: 2.6534 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 143/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6153 - accuracy: 0.7464 - val_loss: 2.5946 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 144/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6216 - accuracy: 0.7407 - val_loss: 2.6380 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 145/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6191 - accuracy: 0.7382 - val_loss: 2.8326 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 146/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6093 - accuracy: 0.7496 - val_loss: 2.6230 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 147/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6087 - accuracy: 0.7455 - val_loss: 2.7126 - val_accuracy: 0.3900 - lr: 0.0050\n",
            "Epoch 148/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6092 - accuracy: 0.7431 - val_loss: 2.7062 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 149/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6174 - accuracy: 0.7391 - val_loss: 2.7017 - val_accuracy: 0.3650 - lr: 0.0050\n",
            "Epoch 150/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6157 - accuracy: 0.7496 - val_loss: 2.6940 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 151/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.6006 - accuracy: 0.7504 - val_loss: 2.7665 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 152/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5858 - accuracy: 0.7699 - val_loss: 2.7979 - val_accuracy: 0.3800 - lr: 0.0050\n",
            "Epoch 153/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6011 - accuracy: 0.7650 - val_loss: 2.7065 - val_accuracy: 0.3450 - lr: 0.0050\n",
            "Epoch 154/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6198 - accuracy: 0.7423 - val_loss: 2.6784 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 155/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5802 - accuracy: 0.7601 - val_loss: 2.7529 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 156/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6086 - accuracy: 0.7431 - val_loss: 2.7081 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 157/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5897 - accuracy: 0.7528 - val_loss: 2.6497 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 158/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5947 - accuracy: 0.7634 - val_loss: 2.6878 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 159/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5841 - accuracy: 0.7690 - val_loss: 2.8421 - val_accuracy: 0.3600 - lr: 0.0050\n",
            "Epoch 160/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5907 - accuracy: 0.7520 - val_loss: 2.6626 - val_accuracy: 0.3700 - lr: 0.0050\n",
            "Epoch 161/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5829 - accuracy: 0.7658 - val_loss: 2.7459 - val_accuracy: 0.3850 - lr: 0.0050\n",
            "Epoch 162/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5623 - accuracy: 0.7666 - val_loss: 2.7389 - val_accuracy: 0.3750 - lr: 0.0050\n",
            "Epoch 163/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5767 - accuracy: 0.7674 - val_loss: 2.6432 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 164/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5994 - accuracy: 0.7561 - val_loss: 2.7950 - val_accuracy: 0.3950 - lr: 0.0050\n",
            "Epoch 165/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.6838 - accuracy: 0.7058 - val_loss: 2.6295 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 166/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.6303 - accuracy: 0.7431 - val_loss: 2.7423 - val_accuracy: 0.3550 - lr: 0.0050\n",
            "Epoch 167/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5822 - accuracy: 0.7682 - val_loss: 2.6421 - val_accuracy: 0.3400 - lr: 0.0050\n",
            "Epoch 168/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5726 - accuracy: 0.7755 - val_loss: 2.7144 - val_accuracy: 0.3350 - lr: 0.0050\n",
            "Epoch 169/700\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.7618\n",
            "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5723 - accuracy: 0.7618 - val_loss: 2.8725 - val_accuracy: 0.3500 - lr: 0.0050\n",
            "Epoch 170/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5567 - accuracy: 0.7812 - val_loss: 2.8114 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 171/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5363 - accuracy: 0.7836 - val_loss: 2.8317 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 172/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5377 - accuracy: 0.7804 - val_loss: 2.8310 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 173/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5341 - accuracy: 0.8023 - val_loss: 2.8637 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 174/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5252 - accuracy: 0.8023 - val_loss: 2.8280 - val_accuracy: 0.3400 - lr: 0.0025\n",
            "Epoch 175/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5300 - accuracy: 0.7958 - val_loss: 2.8859 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 176/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5238 - accuracy: 0.7853 - val_loss: 2.9400 - val_accuracy: 0.3450 - lr: 0.0025\n",
            "Epoch 177/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5288 - accuracy: 0.7958 - val_loss: 2.9703 - val_accuracy: 0.3650 - lr: 0.0025\n",
            "Epoch 178/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5146 - accuracy: 0.8063 - val_loss: 2.9033 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 179/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5154 - accuracy: 0.7974 - val_loss: 2.9224 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 180/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5169 - accuracy: 0.8063 - val_loss: 2.8670 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 181/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5160 - accuracy: 0.7925 - val_loss: 2.9109 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 182/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5226 - accuracy: 0.8006 - val_loss: 3.0374 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 183/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5165 - accuracy: 0.8096 - val_loss: 2.9802 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 184/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5067 - accuracy: 0.7998 - val_loss: 2.9404 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 185/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5103 - accuracy: 0.8006 - val_loss: 2.9655 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 186/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5113 - accuracy: 0.7998 - val_loss: 3.0213 - val_accuracy: 0.3850 - lr: 0.0025\n",
            "Epoch 187/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5078 - accuracy: 0.8055 - val_loss: 3.0165 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 188/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5144 - accuracy: 0.7998 - val_loss: 2.9816 - val_accuracy: 0.3750 - lr: 0.0025\n",
            "Epoch 189/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5087 - accuracy: 0.8006 - val_loss: 2.9604 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 190/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5088 - accuracy: 0.8096 - val_loss: 3.0332 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 191/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5066 - accuracy: 0.8055 - val_loss: 2.9558 - val_accuracy: 0.3450 - lr: 0.0025\n",
            "Epoch 192/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5083 - accuracy: 0.8006 - val_loss: 2.9475 - val_accuracy: 0.3650 - lr: 0.0025\n",
            "Epoch 193/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5097 - accuracy: 0.8136 - val_loss: 3.1244 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 194/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5033 - accuracy: 0.8071 - val_loss: 3.1197 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 195/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5070 - accuracy: 0.8023 - val_loss: 3.1027 - val_accuracy: 0.3900 - lr: 0.0025\n",
            "Epoch 196/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5040 - accuracy: 0.8039 - val_loss: 3.0340 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 197/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.5007 - accuracy: 0.8079 - val_loss: 3.0283 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 198/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.5126 - accuracy: 0.7925 - val_loss: 3.1329 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 199/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4941 - accuracy: 0.8112 - val_loss: 3.1565 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 200/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5015 - accuracy: 0.8071 - val_loss: 3.2723 - val_accuracy: 0.3850 - lr: 0.0025\n",
            "Epoch 201/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5357 - accuracy: 0.7804 - val_loss: 3.2112 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 202/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5064 - accuracy: 0.8015 - val_loss: 3.0582 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 203/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4897 - accuracy: 0.8193 - val_loss: 3.1150 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 204/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4928 - accuracy: 0.8039 - val_loss: 3.0708 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 205/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4869 - accuracy: 0.8306 - val_loss: 3.0936 - val_accuracy: 0.3800 - lr: 0.0025\n",
            "Epoch 206/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4872 - accuracy: 0.8088 - val_loss: 3.0324 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 207/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4891 - accuracy: 0.8023 - val_loss: 3.0724 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 208/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4966 - accuracy: 0.7974 - val_loss: 3.1542 - val_accuracy: 0.3750 - lr: 0.0025\n",
            "Epoch 209/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4919 - accuracy: 0.8096 - val_loss: 3.1105 - val_accuracy: 0.3750 - lr: 0.0025\n",
            "Epoch 210/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.4836 - accuracy: 0.8169 - val_loss: 3.1437 - val_accuracy: 0.3800 - lr: 0.0025\n",
            "Epoch 211/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4848 - accuracy: 0.8104 - val_loss: 3.1692 - val_accuracy: 0.3850 - lr: 0.0025\n",
            "Epoch 212/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.5060 - accuracy: 0.7934 - val_loss: 3.0857 - val_accuracy: 0.3800 - lr: 0.0025\n",
            "Epoch 213/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4889 - accuracy: 0.8088 - val_loss: 3.3299 - val_accuracy: 0.3650 - lr: 0.0025\n",
            "Epoch 214/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4851 - accuracy: 0.8112 - val_loss: 3.1657 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 215/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.4761 - accuracy: 0.8185 - val_loss: 3.2524 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 216/700\n",
            "78/78 [==============================] - 8s 101ms/step - loss: 0.4825 - accuracy: 0.8152 - val_loss: 3.1694 - val_accuracy: 0.3700 - lr: 0.0025\n",
            "Epoch 217/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.4801 - accuracy: 0.8201 - val_loss: 3.2044 - val_accuracy: 0.3600 - lr: 0.0025\n",
            "Epoch 218/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.4869 - accuracy: 0.8120 - val_loss: 3.1752 - val_accuracy: 0.3550 - lr: 0.0025\n",
            "Epoch 219/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4779 - accuracy: 0.8079 - val_loss: 3.2832 - val_accuracy: 0.3750 - lr: 0.0025\n",
            "Epoch 220/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4797 - accuracy: 0.8096 - val_loss: 3.2900 - val_accuracy: 0.3800 - lr: 0.0025\n",
            "Epoch 221/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4804 - accuracy: 0.8128 - val_loss: 3.2435 - val_accuracy: 0.3650 - lr: 0.0025\n",
            "Epoch 222/700\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.8177\n",
            "Epoch 00222: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4806 - accuracy: 0.8177 - val_loss: 3.1617 - val_accuracy: 0.3500 - lr: 0.0025\n",
            "Epoch 223/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4640 - accuracy: 0.8241 - val_loss: 3.2883 - val_accuracy: 0.3800 - lr: 0.0012\n",
            "Epoch 224/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4551 - accuracy: 0.8290 - val_loss: 3.2845 - val_accuracy: 0.3950 - lr: 0.0012\n",
            "Epoch 225/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4528 - accuracy: 0.8371 - val_loss: 3.2589 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 226/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4557 - accuracy: 0.8371 - val_loss: 3.2409 - val_accuracy: 0.4100 - lr: 0.0012\n",
            "Epoch 227/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4535 - accuracy: 0.8241 - val_loss: 3.3049 - val_accuracy: 0.4100 - lr: 0.0012\n",
            "Epoch 228/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4557 - accuracy: 0.8331 - val_loss: 3.2839 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 229/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4531 - accuracy: 0.8274 - val_loss: 3.2784 - val_accuracy: 0.3950 - lr: 0.0012\n",
            "Epoch 230/700\n",
            "78/78 [==============================] - 8s 98ms/step - loss: 0.4501 - accuracy: 0.8387 - val_loss: 3.3172 - val_accuracy: 0.3850 - lr: 0.0012\n",
            "Epoch 231/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4543 - accuracy: 0.8428 - val_loss: 3.2739 - val_accuracy: 0.3600 - lr: 0.0012\n",
            "Epoch 232/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4493 - accuracy: 0.8298 - val_loss: 3.3626 - val_accuracy: 0.3750 - lr: 0.0012\n",
            "Epoch 233/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4559 - accuracy: 0.8379 - val_loss: 3.3125 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 234/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4473 - accuracy: 0.8460 - val_loss: 3.2859 - val_accuracy: 0.3550 - lr: 0.0012\n",
            "Epoch 235/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4514 - accuracy: 0.8331 - val_loss: 3.3499 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 236/700\n",
            "78/78 [==============================] - 8s 101ms/step - loss: 0.4452 - accuracy: 0.8404 - val_loss: 3.2917 - val_accuracy: 0.3850 - lr: 0.0012\n",
            "Epoch 237/700\n",
            "78/78 [==============================] - 8s 102ms/step - loss: 0.4477 - accuracy: 0.8404 - val_loss: 3.3009 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 238/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4479 - accuracy: 0.8379 - val_loss: 3.3773 - val_accuracy: 0.3900 - lr: 0.0012\n",
            "Epoch 239/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4495 - accuracy: 0.8412 - val_loss: 3.3432 - val_accuracy: 0.3500 - lr: 0.0012\n",
            "Epoch 240/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4454 - accuracy: 0.8404 - val_loss: 3.3223 - val_accuracy: 0.3550 - lr: 0.0012\n",
            "Epoch 241/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4432 - accuracy: 0.8501 - val_loss: 3.3087 - val_accuracy: 0.3300 - lr: 0.0012\n",
            "Epoch 242/700\n",
            "78/78 [==============================] - 8s 101ms/step - loss: 0.4446 - accuracy: 0.8412 - val_loss: 3.3569 - val_accuracy: 0.3800 - lr: 0.0012\n",
            "Epoch 243/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4481 - accuracy: 0.8331 - val_loss: 3.3184 - val_accuracy: 0.3300 - lr: 0.0012\n",
            "Epoch 244/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4414 - accuracy: 0.8428 - val_loss: 3.3139 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 245/700\n",
            "78/78 [==============================] - 8s 99ms/step - loss: 0.4435 - accuracy: 0.8485 - val_loss: 3.3392 - val_accuracy: 0.3700 - lr: 0.0012\n",
            "Epoch 246/700\n",
            "78/78 [==============================] - 8s 102ms/step - loss: 0.4435 - accuracy: 0.8395 - val_loss: 3.3590 - val_accuracy: 0.3850 - lr: 0.0012\n",
            "Epoch 247/700\n",
            "78/78 [==============================] - 8s 100ms/step - loss: 0.4391 - accuracy: 0.8460 - val_loss: 3.4109 - val_accuracy: 0.3600 - lr: 0.0012\n",
            "Epoch 248/700\n",
            " 4/78 [>.............................] - ETA: 6s - loss: 0.4798 - accuracy: 0.8125"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-e8df8bdd8744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs = 700, batch_size = 16, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_data= (X_test, y_test))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,5))\n",
        "plt.plot(history.history['loss'], label='train', marker='*')\n",
        "plt.plot(history.history['val_loss'], label='validation', marker='*')\n",
        "plt.grid()\n",
        "plt.legend(fontsize=11)"
      ],
      "metadata": {
        "id": "tRPW3p7gs6VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "rx07WIKBtAK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = folder + 'modelo_best.h5'\n",
        "checkpoint = ModelCheckpoint(model_path, monitor=\"val_loss\",\n",
        "                             save_freq = 'epoch', save_best_only=True, mode='min') "
      ],
      "metadata": {
        "id": "CbasMtrhs-xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Very low learning rate \n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics='accuracy') \n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 100, batch_size = 16,\n",
        "                    validation_data= (X_test, y_test),\n",
        "                    callbacks=[early_stopping, checkpoint])"
      ],
      "metadata": {
        "id": "pzv0swFxtDJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "058bf41a-c428-4d10-a4be-17ffef15f474"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 54s 171ms/step - loss: 6.1896 - accuracy: 0.4336 - val_loss: 1.3790 - val_accuracy: 0.3600\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 11s 140ms/step - loss: 1.0004 - accuracy: 0.5123 - val_loss: 1.4159 - val_accuracy: 0.3200\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 11s 139ms/step - loss: 0.9726 - accuracy: 0.5361 - val_loss: 1.4114 - val_accuracy: 0.3850\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 11s 142ms/step - loss: 0.9470 - accuracy: 0.5598 - val_loss: 1.4665 - val_accuracy: 0.3700\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 11s 141ms/step - loss: 0.8901 - accuracy: 0.5754 - val_loss: 1.6790 - val_accuracy: 0.3300\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 14s 185ms/step - loss: 0.9030 - accuracy: 0.5820 - val_loss: 1.5817 - val_accuracy: 0.3450\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 11s 143ms/step - loss: 0.8128 - accuracy: 0.6090 - val_loss: 2.1243 - val_accuracy: 0.3650\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 11s 141ms/step - loss: 0.7912 - accuracy: 0.6336 - val_loss: 2.0033 - val_accuracy: 0.3000\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 11s 142ms/step - loss: 0.7826 - accuracy: 0.6434 - val_loss: 1.9166 - val_accuracy: 0.3000\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 11s 142ms/step - loss: 0.8145 - accuracy: 0.6164 - val_loss: 1.5234 - val_accuracy: 0.3800\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 11s 142ms/step - loss: 0.6851 - accuracy: 0.6885 - val_loss: 1.9712 - val_accuracy: 0.3150\n",
            "Epoch 12/100\n",
            "28/77 [=========>....................] - ETA: 6s - loss: 0.6475 - accuracy: 0.7277"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0c7a380cedfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m history = model.fit(X_train, y_train, epochs = 100, batch_size = 16,\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     callbacks=[early_stopping, checkpoint])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,5))\n",
        "plt.plot(history.history['loss'], label='train', marker='*')\n",
        "plt.plot(history.history['val_loss'], label='validation', marker='*')\n",
        "plt.grid()\n",
        "plt.legend(fontsize=11)"
      ],
      "metadata": {
        "id": "GLr8j9H4tFO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tf.keras.models.load_model(model_path)\n",
        "best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "HOjsrt_ctHr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "kbZORO4utXFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = best_model.predict(X_test, verbose=1)\n",
        "y_pred_redes = best_model.predict(X_new, verbose=1)"
      ],
      "metadata": {
        "id": "JHrw0hfatQo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "axgisN12tdWD"
      }
    }
  ]
}